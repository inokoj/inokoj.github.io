<!DOCTYPE html>
<!--[if IE 7]>
<html class="ie ie7" lang="ja">
<![endif]-->
<!--[if IE 8]>
<html class="ie ie8" lang="ja">
<![endif]-->
<!--[if !(IE 7) & !(IE 8)]><!-->
<html lang="ja">
<!--<![endif]-->
<head>
<base href="https://inokoj.github.io" />

<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />
<title>Publications | Koji Inoue</title>


<!--[if lt IE 9]>
<script src="https://inokoj.github.io/wp-content/themes/twentytwelve/js/html5.js" type="text/javascript"></script>
<![endif]-->



<link rel='stylesheet' id='twentytwelve-fonts-css'  href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,400,700&#038;subset=latin,latin-ext' type='text/css' media='all' />
<link rel='stylesheet' id='twentytwelve-style-css'  href='https://inokoj.github.io/wp-content/themes/twentytwelve/style.css?ver=4.1.1' type='text/css' media='all' />
<!--[if lt IE 9]>
<link rel='stylesheet' id='twentytwelve-ie-css'  href='https://inokoj.github.io/wp-content/themes/twentytwelve/css/ie.css?ver=20121010' type='text/css' media='all' />
<![endif]-->
<script type='text/javascript' src='https://inokoj.github.io/wp-includes/js/jquery/jquery.js?ver=1.11.1'></script>
<script type='text/javascript' src='https://inokoj.github.io/wp-includes/js/jquery/jquery-migrate.min.js?ver=1.2.1'></script>

 

<link rel='canonical' href='https://inokoj.github.io/publications.html' />
<link rel='shortlink' href='https://inokoj.github.io/?p=8' />
</head>

<body class="page page-id-8 page-template-default full-width custom-font-enabled single-author">
<div id="page" class="hfeed site">
	<header id="masthead" class="site-header" role="banner">
		<hgroup>
			<h1 class="site-title"><a href="https://inokoj.github.io/" title="Koji Inoue" rel="home">Koji Inoue</a></h1>
			<h2 class="site-description">Speech and Audio Processing Laboratory &#8211; Kyoto University</h2>
		</hgroup>

		<nav id="site-navigation" class="main-navigation" role="navigation">
			<button class="menu-toggle">メニュー</button>
			<a class="assistive-text" href="#content" title="コンテンツへ移動">コンテンツへ移動</a>
			<div class="menu-main-menu-container"><ul id="menu-main-menu" class="nav-menu"><li id="menu-item-15" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-15"><a href="https://inokoj.github.io/">Top</a></li>
<li id="menu-item-18" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-18"><a href="https://inokoj.github.io/profile.html">About me</a></li>
<li id="menu-item-12" class="menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-8 current_page_item menu-item-12"><a href="https://inokoj.github.io/publications.html">Publications</a></li>
</ul></div>		</nav><!-- #site-navigation -->

			</header><!-- #masthead -->

	<div id="main" class="wrapper">
	<div id="primary" class="site-content">
		<div id="content" role="main">

							
	<article id="post-8" class="post-8 page type-page status-publish hentry">
		<header class="entry-header">
												<h1 class="entry-title">Publications</h1>
		</header>

<div class="entry-content">
<p><span style="font-size: 10pt;">
<a href="https://inokoj.github.io/publications.html#journal">学術雑誌 (Journal papers)</a> /
 <a href="https://inokoj.github.io/publications.html#invited">招待講演 (Invited talk)</a> /
 <a href="https://inokoj.github.io/publications.html#conference_i">国際会議 (International conference)</a> /
 <a href="https://inokoj.github.io/publications.html#conference_j">全国大会・研究会</a> /
 <a href="https://inokoj.github.io/publications.html#kaisetsu">学会誌</a> /
 <a href="https://inokoj.github.io/publications.html#other">その他</a></span>
</p>
<p><span style="font-size: 10pt;"><a href="https://scholar.google.co.jp/citations?user=qljXMcMAAAAJ&hl=ja" target="_blank">Google Scholar</a> / <a href="https://researchmap.jp/kojiinoue/?lang=japanese" target="_blank">Research map</a></span></p>

<hr />
<p><span style="font-size: 10pt;"><strong><a id="journal"></a>学術雑誌 (Journal papers)</strong></span></p>

<ul style="list-style-type: disc;">
	<li>
	Kenta Yamamoto, <strong>Koji Inoue</strong>, Tatsuya Kawahara.<br>
	Character expression of a conversational robot for adapting to user personality.<br>
	Advanced Robotics, Vol. 38, No. 24, pp. 256-266, 2023. <br>
	[<a href="https://www.tandfonline.com/doi/full/10.1080/01691864.2023.2285804" target="_blank">Link</a>][<a href="https://repository.kulib.kyoto-u.ac.jp/dspace/handle/2433/287441" target="_blank">Preprint PDF</a>]<br>
	</li>
	</ul>

<ul style="list-style-type: disc;">
	<li>
	Yahui Fu, <strong>Koji Inoue</strong>, Divesh Lala, Kenta Yamamoto, Chenhui Chu, Tatsuya Kawahara.<br>
	Dual variational generative model and auxiliary retrieval for empathetic response generation by conversational robot.<br>
	Advanced Robotics, Vol. 37, No. 21, pp. 1406-1418, 2023. <br>
	[<a href="https://www.tandfonline.com/doi/full/10.1080/01691864.2023.2270577" target="_blank">Link</a>][<a href="https://repository.kulib.kyoto-u.ac.jp/dspace/handle/2433/285997" target="_blank">Preprint PDF</a>]<br>
	</li>
	</ul>

<ul style="list-style-type: disc;">
	<li>
	Keiko Ochi, <strong>Koji Inoue</strong>, Divesh Lala, Tatsuya Kawahara, Hirokazu Kumazaki.<br>
	Effect of attentive listening robot on pleasure and arousal change in psychiatric daycare.<br>
	Advanced Robotics, Vol. 37, No. 21, pp. 1382-1391, 2023. <br>
	[<a href="https://www.tandfonline.com/doi/full/10.1080/01691864.2023.2257264" target="_blank">Link</a>][<a href="https://repository.kulib.kyoto-u.ac.jp/dspace/handle/2433/285730" target="_blank">Preprint PDF</a>]<br>
	</li>
	</ul>
<ul style="list-style-type: disc;">
<li>
Kenta Yamamoto, <strong>Koji Inoue</strong>, Tatsuya Kawahara.<br>
Character expression for spoken dialogue systems with semi-supervised learning using Variational Auto-Encoder.<br>
Computer Speech & Language, Vol. 79, 101469, 2022. <br>
[<a href="https://www.sciencedirect.com/science/article/pii/S0885230822000924?via%3Dihub" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>Koji Inoue</strong>, Divesh Lala, Tatsuya Kawahara.<br>
Can a robot laugh with you?: Shared laughter generation for empathetic spoken dialogue.<br>
Frontiers in Robotics and AI, 2022. <br>
[<a href="https://www.frontiersin.org/articles/10.3389/frobt.2022.933261/full" target="_blank">Link</a>] [<a href="https://inokoj.github.io/slg/" target="_blank">Demo page</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>井上昂治</strong>, ラーラー ディベッシュ, 山本賢太, 中村静, 高梨克也, 河原達也.<br>
アンドロイドERICAの傾聴対話システム--人間による傾聴との比較評価--.<br>
人工知能学会論文誌, Vol. 36, No. 5, pp. H-L51_1-12, 2021. <br>
[<a href="https://www.jstage.jst.go.jp/article/tjsai/36/5/36_36-5_H-L51/_article/-char/ja" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Tatsuya Kawahara, Naoyuki Muramatsu, Kenta Yamamoto, Divesh Lala, <strong>Koji Inoue</strong>.<br>
Semi-autonomous avatar enabling unconstrained parallel conversations --seamless hybrid of WOZ and autonomous dialogue systems--.<br>
Advanced Robotics, Vol. 35, No. 11, pp. 657-663, 2021.<br>
[<a href="https://www.tandfonline.com/doi/full/10.1080/01691864.2021.1928549?scroll=top&needAccess=true" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>井上昂治</strong>, 原康平, Divesh Lala, 山本賢太, 中村静, 高梨克也, 河原達也.<br>
掘り下げ質問を行う就職面接対話システムの自律型アンドロイドでの実装と評価.<br>
人工知能学会論文誌, Vol. 35, No. 5, pp. D-K43_1-10, 2020.<br>
[<a href="https://www.jstage.jst.go.jp/article/tjsai/35/5/35_35-5_D-K43/_article/-char/ja" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Yuanchao Li, Carlos T. Ishi, <strong>Koji Inoue</strong>, Shizuka Nakamura, Tatsuya Kawahara.<br>
Expressing reactive emotion based on multimodal emotion recognition for natural conversation in human-robot interaction.<br>
Advanced Robotics, Vol. 33, No 20, pp. 1030-1041, 2019.<br>
[<a href="https://www.tandfonline.com/doi/full/10.1080/01691864.2019.1667872" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>Koji Inoue</strong>, Divesh Lala, Katsuya Takanashi, Tatsuya Kawahara.<br>
Engagement recognition by a latent character model based on multimodal listener behaviors in spoken dialogue.<br>
APSIPA Transaction on Signal and Information Processing, Vol. 7, No. e9, pp. 1-16, 2018.<br>
[<a href="https://www.cambridge.org/core/journals/apsipa-transactions-on-signal-and-information-processing/article/engagement-recognition-by-a-latent-character-model-based-on-multimodal-listener-behaviors-in-spoken-dialogue/E520CBA5B07362373D362F8CAD3E4696" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
山本賢太, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也.<br>
人間型ロボットのキャラクタ表現のための対話の振る舞い制御モデル.<br>
人工知能学会論文誌, Vol. 33, No. 5, pp. C-l37_1-9, 2018. <br>
[<a href="https://doi.org/10.1527/tjsai.C-I37" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>井上昂治</strong>, Divesh Lala, 吉井和佳, 高梨克也, 河原達也.<br>
潜在キャラクタモデルによる聞き手のふるまいに基づく対話エンゲージメントの推定.<br>
人工知能学会論文誌, Vol. 33, No. 1, pp. DSH-F_1-12, 2018.<br>
[<a href="https://doi.org/10.1527/tjsai.DSH-F" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
山口貴史, <strong>井上昂治</strong>, 吉野幸一郎, 高梨克也, Nigel G. Ward, 河原達也.<br>
傾聴対話システムのための言語情報と韻律情報に基づく多様な形態の相槌の生成.<br>
工知能学会論文誌, Vol. 31, No. 4, pp. C-G31_1-10, 2016.<br>
[<a href="https://www.jstage.jst.go.jp/article/tjsai/31/4/31_C-G31/_article/-char/ja/" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Tatsuya Kawahara, Takuma Iwatate, <strong>Koji Inoue</strong>, Soichiro Hayashi, Hiromasa Yoshimoto, Katsuya Takanashi.<br>
Multi-modal sensing and analysis of poster conversations with smart posterboard.<br>
APSIPA Transaction on Signal and Information Processing, Vol. 5, No. e2, pp. 1-12, 2016.<br>
[<a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=10207240&fileId=S2048770316000020" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>井上昂治</strong>, 若林佑幸, 吉本廣雅, 河原達也.<br>
多人数会話における音響・視線情報を統合した話者区間検出.<br>
電子情報通信学会論文誌, Vol. J99-D, No. 3, pp. 348-357, 2016.<br>
[<a href="http://search.ieice.org/bin/summary.php?id=j99-d_3_348&category=D&year=2016&lang=J&abst=" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
若林佑幸, <strong>井上昂治</strong>, 中山雅人, 西浦敬信, 山下洋一, 吉本廣雅, 河原達也.<br>
視聴覚情報の統合に基づく音源数推定と話者ダイアライゼーション.<br>
電子情報通信学会論文誌, Vol. J99-D, No. 3, pp. 326-336, 2016.<br>
[<a href="http://search.ieice.org/bin/summary.php?id=j99-d_3_326&category=D&lang=J&year=2016&abst=" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>Koji Inoue</strong>, Kohei Isechi, Hironobu Saito, Yoshimitsu Kuroki.<br>
An Inter-Prediction Method using Sparse Representation for High Efficiency Video Coding.<br>
IEICE Trans. on Fundamentals of Electronics, Communications and Computer Sciences, Vol. E96-A, No. 11, pp. 2191-2193, 2013.<br>
[<a href="http://search.ieice.org/bin/summary.php?id=e96-a_11_2191&amp;category=A&amp;year=2013&amp;lang=E&amp;abst=" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>Kenjiro Sugimoto, <strong>Koji Inoue</strong>, Yoshimitsu Kuroki, Sei-ichiro Kamata.<br>
A linear manifold color descriptor for medicine package recognition.<br>
IEICE Trans. on Information and Systems, Vol. E95-D, No. 5, pp. 1264-1271, 2012.<br>
[<a href="http://search.ieice.org/bin/summary.php?id=e95-d_5_1264&amp;category=D&amp;year=2012&amp;lang=E&amp;abst=" target="_blank">Link</a>]
</li>
</ul>
<hr />

<p><span style="font-size: 10pt;"><b><a id="invited"></a>招待講演 (Invited talk)</b></span></p>

<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong>.<br>
	Yeah, Well, Haha: Generating Non-linguistic Behaviors For Human-like Conversational Robots.<br>
	SIGdial Meeting on Discourse and Dialogue (SIGDIAL), 2024.<br>
	[<a href="https://2024.sigdial.org/keynotes/#koji" target="_blank">Link</a>]
	</span>
	</li>
</ul>

<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;"><strong>井上昂治</strong>.<br>
	音声対話の魅力：相槌・笑い・ターンテイキング.<br>
	日本音響学会 音声研究会, 電子情報通信学会 VNV研究会, 2024.<br>
	</span>
	</li>
</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong>.<br>
		Closing the Gap: Exploring Human-Level Interaction in Android Robot Dialogue Systems.<br>
	IEEE RO-MAN Workshop, Multidisciplinary Perspectives on COntext-aware embodied Spoken Interactions (MP-COSIN), 2023.<br>
	[<a href="https://www.mpcosin.com/" target="_blank">Link</a>]
	</span>
	</li>
</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;"><strong>井上昂治</strong>.<br>
	アンドロイドERICAの音声対話システム～マルチモーダルチューリングテストへの挑戦～.<br>
	音学シンポジウム, 2021.<br>
	</span>
	</li>
</ul>
<ul style="list-style-type: disc;">
	<li><strong>井上昂治</strong>.<br>
	空気が読める会話ロボット.<br>
	情報処理学会全国大会 IPSJ-ONE 2019, March. 2019.<br>
	[<a href="https://ipsj-one.org/2019/" target="_blank">Link (講演動画あり)</a>] [<a href="https://www.milive-plus.net/gakumon191005/" target="_blank">特集記事</a>]
</span></li>
</ul>
<hr />

<p><span style="font-size: 10pt;"><strong><a id="conference_i"></a>国際会議 (International conference)</strong></span></p>

<ul style="list-style-type: disc;">
	<li>
		Keiko Ochi, <strong>Koji Inoue</strong>, Divesh Lala, Tatsuya Kawahara.<br>
		Entrainment Analysis and Prosody Prediction of Subsequent Interlocutor’s Backchannels in Dialogue.<br>
		INTERSPEECH, pp. 462-466, 2024.<br>
		[<a href="https://www.isca-archive.org/interspeech_2024/ochi24_interspeech.html" target="_blank">Link</a>]<br>
	</li>
</ul>

<ul style="list-style-type: disc;">
	<li>
		<strong>Koji Inoue</strong>, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel Skantze.<br>
		Multilingual Turn-taking Prediction Using Voice Activity Projection.<br>
		Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING), pp. 11873-11883, 2024.<br>
		[<a href="https://aclanthology.org/2024.lrec-main.1036/" target="_blank">Link</a>][<a href="https://arxiv.org/abs/2403.06487" target="_blank">Preprint PDF</a>]<br>
	</li>
</ul>

<ul style="list-style-type: disc;">
	<li>
		<strong>Koji Inoue</strong>, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel Skantze.<br>
		Real-time and Continuous Turn-taking Prediction Using Voice Activity Projection.<br>
		International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2024.<br>
		[<a href="https://arxiv.org/abs/2401.04868" target="_blank">Preprint PDF</a>]<br>
	</li>
</ul>

<ul style="list-style-type: disc;">
	<li>
		<strong>Koji Inoue</strong>, Divesh Lala, Keiko Ochi, Tatsuya Kawahara, Gabriel Skantze.<br>
		An Analysis of User Behaviors for Objectively Evaluating Spoken Dialogue Systems.<br>
		International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2024.<br>
		[<a href="https://arxiv.org/abs/2401.04867" target="_blank">Preprint PDF</a>]<br>
	</li>
</ul>

<ul style="list-style-type: disc;">
	<li>
		Zi Haur Pang, Yahui Fu, Divesh Lala, Keiko Ochi, <strong>Koji Inoue</strong>, Tatsuya Kawahara.<br>
		Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue.<br>
		International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2024.<br>
		[<a href="https://arxiv.org/abs/2402.12770" target="_blank">Preprint PDF</a>]<br>
	</li>
</ul>

<ul style="list-style-type: disc;">
	<li>
		Haruki Kawai, Divesh Lala, <strong>Koji Inoue</strong>, Keiko Ochi, Tatsuya Kawahara.<br>
		Evaluation of a semi-autonomous attentive listening system with takeover prompting.<br>
		International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2024.<br>
		[<a href="https://arxiv.org/abs/2402.14863" target="_blank">Preprint PDF</a>]<br>
	</li>
</ul>

<ul style="list-style-type: disc;">
	<li>
		Sanae Yamashita, <strong>Koji Inoue</strong>, Ao Guo, Shota Mochizuki, Tatsuya Kawahara, Ryuichiro Higashinaka.<br>
		RealPersonaChat: A Realistic Persona Chat Corpus with Interlocutors’ Own Personalities.<br>
		Pacific Asia Conference on Language, Information and Computation (PACLIC), pp. 852–861, 2023.<br>
		[<a href="https://aclanthology.org/2023.paclic-1.85/" target="_blank">Link</a>]<br>
	</li>
</ul>

<ul style="list-style-type: disc;">
	<li>
		<strong>Koji Inoue</strong>, Divesh Lala, Keiko Ochi, Tatsuya Kawahara, Gabriel Skantze.<br>
		Towards Objective Evaluation of Socially-Situated Conversational Robots: Assessing Human-Likeness through Multimodal User Behaviors.<br>
		International Conference on Multimodal Interaction (ICMI), Late-Breaking Results, Companion Proceedings, pp. 86–90, 2023.<br>
		[<a href="https://dl.acm.org/doi/abs/10.1145/3610661.3617151" target="_blank">Link</a>][<a href="https://arxiv.org/abs/2308.11020" target="_blank">Preprint PDF</a>]<br>
	</li>
</ul>
<ul style="list-style-type: disc;">
	<li>
		<strong>Koji Inoue</strong>.<br>
		Challenges and Approaches in Designing Social SDS in the LLM Era.<br>
		Young Researchers Roundtable on Spoken Dialogue Systems (YRRSDS), 2023.<br>
		[<a href="https://aclanthology.org/2023.yrrsds-1.8/" target="_blank">Link</a>]<br>
	</li>
</ul>
<ul style="list-style-type: disc;">
	<li>
		Yahui Fu, <strong>Koji Inoue</strong>, Chenhui Chu, Tatsuya Kawahara.<br>
		Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation.<br>
		SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pp. 645–656, 2023.<br>
		[<a href="https://aclanthology.org/2023.sigdial-1.60/" target="_blank">Link</a>][<a href="https://arxiv.org/abs/2308.00085" target="_blank">Preprint PDF</a>]<br>
	</li>
</ul>
<ul style="list-style-type: disc;">
	<li>
		Sota Kobuki, Katie Seaborn, Seiki Tokunaga, Kosuke Fukumori, Shun Hidaka, Kazuhiro Tamura, <strong>Koji Inoue</strong>, Tatsuya Kawahara, Mihoko Otake-Matsuura. <br>
		Robotic Backchanneling in Online Conversation Facilitation: A Cross-Generational Study.<br>
		International Conference on Robot and Human Interactive Communication (RO-MAN), 2023.<br>
		[<a href="https://ieeexplore.ieee.org/abstract/document/10309362" target="_blank">Link</a>]<br>
	</li>
</ul>
<ul style="list-style-type: disc;">
	<li>
		Yuanchao Li, <strong>Koji Inoue</strong>, Leimin Tian, Changzeng Fu, Carlos Toshinori Ishi, Hiroshi Ishiguro, Tatsuya Kawahara, Catherine Lai.<br>
		I Know Your Feelings Before You Do: Predicting Future Affective Reactions in Human-Computer Dialogue.<br>
		CHI Conference on Human Factors in Computing Systems, Late-Breaking Work, 2023.<br> 
		[<a href="https://dl.acm.org/doi/abs/10.1145/3544549.3585869" target="_blank">Link</a>]
	</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Kenta Yamamoto, <strong>Koji Inoue</strong>, Tatsuya Kawahara.<br>
Character adaptation of spoken dialogue systems based on user personality.<br>
International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2023.<br>
<strong><span style="color: #ff0000;">Best Paper Award</span></strong><br>
[<a href="https://drive.google.com/file/d/1Akt7Le2Hs_duK3JMctv-pDew3qEmmv1Y/view" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Yahui Fu, <strong>Koji Inoue</strong>, Divesh Lala, Kenta Yamamoto, Chenhui Chu, Tatsuya Kawahara.<br>
Improving Empathetic Response Generation with Retrieval based on Emotion Recognition.<br>
International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2023.<br>
[<a href="https://drive.google.com/file/d/1SwG4YlSzmp8p6CaA1-dvlh9oH1KUYy18/view" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Yusuke Muraki, Divesh Lala, Haruki Kawai, Kenta Yamamoto, <strong>Koji Inoue</strong>, Tatsuya Kawahara.<br>
Semi-autonomous Guide Agents with Simultaneous Handling of Multiple Users.<br>
International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2023.<br>
[<a href="https://drive.google.com/file/d/1secQzJdJaWkrCgAi6shoaKQhWxiFOF4N/view" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Divesh Lala, <strong>Koji Inoue</strong>, Kei Sawada, Tatsuya Kawahara.<br>
Backchannel Generation Model for a Third Party Listening Agent.<br>
International Conference on Human-Agent Interaction (HAI), pp. 114-122, 2022.<br>
[<a href="https://dl.acm.org/doi/abs/10.1145/3527188.3561926" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Seiki Tokunaga, Kohsuke Fukumori, Kazuhiro Tamura, <strong>Koji Inoue</strong>, Tatsuya Kawahara Mihoko Ohtake-Matsuura.<br>
Development of RobotHub: Integration of External System to Group Conversation System for Older Adults.<br>
World Conference of Gerontechnology (ISG), 2022.<br>
[<a href="https://journal.gerontechnology.org/archives/6d1a927c48db45d59f7c3ce765a1490a.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Haruki Kawai, Yusuke Muraki, Kenta Yamamoto, Divesh Lala, <strong>Koji Inoue</strong>, Tatsuya Kawahara.<br>
Simultaneous Job Interview System Using Multiple Semi-autonomous Agents.<br>
SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pp. 107-110, 2022.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/KAW-SIGDIAL22.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Seiya Kawano, Muteki Arioka, Akishige Yuguchi, Kenta Yamamoto, <strong>Koji Inoue</strong>, Tatsuya Kawahara, Satoshi Nakamura, Koichiro Yoshino.<br>
Multimodal Persuasive Dialogue Corpus using Teleoperated Android.<br>
INTERSPEECH, pp. 2918-2922, 2022.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/KAW-INTERSP22.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Yuanchao Li, Catherine Lai, Divesh Lala, <strong>Koji Inoue</strong>, Tatsuya Kawahara.<br>
Alzheimer’s Dementia Detection through Spontaneous Dialogue with Proactive Robotic Listeners.<br>
ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 875-879, 2022.<br>
[<a href="https://dl.acm.org/doi/abs/10.5555/3523760.3523896" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>Koji Inoue</strong>, Hiromi Sakamoto, Kenta Yamamoto, Divesh Lala, Tatsuya Kawahara.<br>
A multi-party attentive listening robot which stimulates involvement from side participants.<br>
SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pp. 261-264, 2021.<br>
[<a href="https://sigdial.org/sites/default/files/workshops/conference22/Proceedings/pdf/2021.sigdial-1.28.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Divesh Lala, <strong>Koji Inoue</strong>, Kenta Yamamoto, Tatsuya Kawahara.<br>
Findings from human-android dialogue research with ERICA.<br>
RobotDial Workshop on Dialogue Models for Human-Robot Interaction (ROBOTDIAL), IJCAI-PRICAI workshop, 2021.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/ijcai2020/robotdial/09.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Divesh Lala, <strong>Koji Inoue</strong>, Tatsuya Kawahara.<br>
Prediction of shared laughter for human-robot dialogue.<br>
International Conference on Multimodal Interaction (ICMI), Companion Publication, pp. 62-66, 2020.<br>
[<a href="https://dl.acm.org/doi/10.1145/3395035.3425265" target="_blank">Link</a>][<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LAL-ICMI20.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>Koji Inoue</strong>, Kohei Hara, Divesh Lala, Kenta Yamamoto, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
Job interviewer android with elaborate follow-up question generation.<br>
International Conference on Multimodal Interaction (ICMI), pp. 324-332, 2020.<br>
[<a href="https://dl.acm.org/doi/10.1145/3382507.3418839" target="_blank">Link</a>][<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INO-ICMI20.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Kenta Yamamoto, <strong>Koji Inoue</strong>, Tatsuya Kawahara.<br>
Semi-supervised learning for character expression of spoken dialogue systems.<br>
INTERSPEECH, pp. 4188-4192, 2020.<br>
[<a href="https://www.isca-speech.org/archive/Interspeech_2020/abstracts/2293.html" target="_blank">Link</a>][<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/YAM-INTERSP20.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>Koji Inoue</strong>, Divesh Lala, Kenta Yamamoto, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
An attentive listening system with android ERICA: Comparison of autonomous and WOZ interactions.<br>
SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pp. 118-127, 2020.<br>
[<a href="https://www.aclweb.org/anthology/2020.sigdial-1.15/" target="_blank">Link</a>][<a href="https://www.sigdial.org/files/workshops/conference21/pdf/2020.sigdial-1.15.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Sota Isonishi, <strong>Koji Inoue</strong>, Divesh Lala, Katsuya Takanashi, Tatsuya Kawahara.<br>
Response generation to out-of-database questions for example-based dialogue systems.<br>
International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2020.<br>
[<a href="https://link.springer.com/chapter/10.1007%2F978-981-15-8395-7_23" target="_blank">Link</a>][<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/ISO-IWSDS20.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Kenta Yamamoto, <strong>Koji Inoue</strong>, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
A character expression model affecting spoken dialogue behaviors.<br>
International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2020.<br>
[<a href="http://link-springer-com-443.webvpn.fjmu.edu.cn/chapter/10.1007%2F978-981-15-8395-7_1" target="_blank">Link</a>][<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/YAM-IWSDS20.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Divesh Lala, <strong>Koji Inoue</strong>, Tatsuya Kawahara.<br>
Smooth turn-taking by a robot using an online continuous model to generate turn-taking cues.<br>
International Conference on Multimodal Interaction (ICMI), pp. 226-234, 2019.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LAL-ICMI19.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Kohei Hara, <strong>Koji Inoue</strong>, Katsuya Takanashi, Tatsuya Kawahara.<br>
Turn-taking Prediction Based on Detection of Transition Relevance Place.<br>
INTERSPEECH, pp. 4170-4179, 2019.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/HAR-INTERSP19.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>Koji Inoue</strong>, Divesh Lala, Kenta Yamamoto, Katsuya Takanashi, Tatsuya Kawahara.<br>
Engagement-based adaptive behaviors for laboratory guide in human-robot dialogue.<br>
International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2019.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INO-IWSDS19.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>Koji Inoue</strong>, Kohei Hara, Divesh Lala, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
A job interview dialogue system with autonomous android ERICA.<br>
International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2019.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/HAR-IWSDS19.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Koki Tanaka, <strong>Koji Inoue</strong>, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
End-to-end modeling for selection of utterance constructional units via system internal states.<br>
International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2019.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/TAN-IWSDS19.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Kenta Yamamoto, <strong>Koji Inoue</strong>, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
Dialogue Behavior Control Model for Expressing a Character of Humanoid Robots.<br>
APSIPA ASC, pp. 1732-1737, 2018.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/YAM-APSIPA18.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Divesh Lala, <strong>Koji Inoue</strong>, Tatsuya Kawahara.<br>
Evaluation of Real-time Deep Learning Turn-taking Models for Multiple Dialogue Scenarios.<br>
International Conference on Multimodal Interaction (ICMI), pp. 78-86, 2018.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LAL-ICMI18.pdf" target="_blank">PDF</a>][<a href="https://dl.acm.org/citation.cfm?id=3242994" target="_blank">Link</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>Koji Inoue</strong>, Divesh Lala, Katsuya Takanashi, Tatsuya Kawahara.<br>
Engagement recognition in spoken dialogue via neural network by aggregating different annotators' models.<br>
INTERSPEECH, pp. 616-620, 2018.<br>
[<a href="https://www.isca-speech.org/archive/pdfs/interspeech_2018/inoue18b_interspeech.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Kohei Hara, <strong>Koji Inoue</strong>, Katsuya Takanashi, Tatsuya Kawahara.<br>
Prediction of turn-taking using multitask learning with prediction of backchannels and fillers.<br>
INTERSPEECH, pp. 991-995, 2018.<br>
[<a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1442.pdf" target="_blank">PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
<strong>Koji Inoue</strong>, Divesh Lala, Katsuya Takanashi, Tatsuya Kawahara.<br>
Latent Character Model for Engagement Recognition Based on Multimodal Behaviors.<br>
International Workshop on Spoken Dialogue Systems (IWSDS), 2018.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INO-IWSDS18.pdf" target=blank>PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Ryosuke Nakanishi, <strong>Koji Inoue</strong>, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
Generating Fillers based on Dialog Act Pairs for Smooth Turn-Taking by Humanoid Robot.<br>
International Workshop on Spoken Dialogue Systems (IWSDS), May 2018.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/NAK-IWSDS18.pdf" target=blank>PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Tatsuya Kawahara, <strong>Koji Inoue</strong>, Divesh Lala, Katsuya Takanashi. <br>
Audio-visual conversation analysis by smart posterboard and humanoid robot. <br>
IEEE-ICASSP, pp. 6573-6577, April 2018.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/KAW-ICASSP18.pdf" target=blank>PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li>
Hirofumi Inaguma, Masato Mimura, <strong>Koji Inoue</strong>, Kazuyoshi Yoshii, Tatsuya Kawahara.<br>
An end-to-end approach to joint social signal detection and automatic speech recognition.<br>
IEEE-ICASSP, pp. 6214-6218, April 2018.<br>
[<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INA-ICASSP18.pdf" target=blank>PDF</a>]
</li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Yuanchao Li, Carlos Toshinori Ishi, Nigel Ward, <strong>Koji Inoue</strong>, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara, “Emotion recognition by combining prosody and sentiment analysis for expressing reactive emotion by humanoid robot,” APSIPA ASC, Dec. 2017. [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LYC-APSIPA17.pdf" target=blank>PDF</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Divesh Lala, <strong>Koji Inoue</strong>, Pierrick Milhorat, Tatsuya Kawahara, “Detection of social signals for recognizing engagement in human-robot interaction,” AAAI Fall Symposium Natural Communication for Human-Robot Collaboration (NCHRC), Nov. 2017. [<a href="http://www.ttic.edu/nchrc/papers/6.pdf" target=blank>PDF</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Hirofumi Inaguma, <strong>Koji Inoue</strong>, Masato Mimura, Tatsuya Kawahara, “Social Signal Detection in Spontaneous Dialogue Using Bidirectional LSTM-CTC,” INTERSPEECH, pp. 1691-1695, Aug. 2017. [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INA-INTERSP17.pdf" target=blank>PDF</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Divesh Lala, Milhorat Pierrick, <strong>Koji Inoue</strong>, Masanari Ishida, Tianyu Zhao, Tatsuya Kawahara, “Attentive listening system with backchanneling, response generation and flexible turn-taking,” SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pp. 127-136, Aug. 2017. [<a href="http://www.sigdial.org/workshops/conference18/proceedings/pdf/SIGDIAL16.pdf" target=blank>PDF</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Pierrick Milhorat, Divesh Lala, <strong>Koji Inoue</strong>, Tianyu Zhao, Masanari Ishida, Katsuya Takanashi, Shizuka Nakamura, Tatsuya Kawahara, “A conversational dialogue manager for the humanoid robot ERICA,” International Workshop on Spoken Dialogue Systems (IWSDS), June 2017. [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/MIL-IWSDS17.pdf" target=blank>PDF</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong>, Divesh Lala, Katsuya Takanashi, Tatsuya Kawahara, “Annotation and analysis of listener's engagement based on multi-modal behaviors,” ICMI workshop on Multimodal Analyses enabling Artificial Agents in Human-Machine Interaction (MA3HMI), Nov. 2016. [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INO-ICMI-WS16.pdf" target=blank>PDF</a>][<a href="http://dl.acm.org/citation.cfm?id=3011271" target=blank>Link</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Hirofumi Inaguma, <strong>Koji Inoue</strong>, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara, “Prediction of ice-breaking between participants using prosodic features in the first meeting dialogue,” ICMI workshop on Advancements in Social Signal Processing for Multimodal Interaction (ASSP4MI), Nov. 2016. [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INA-ICMI-WS16.pdf" target=blank>PDF</a>][<a href="http://dl.acm.org/citation.cfm?id=3005472" target=blank>Link</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Divesh Lala, Pierrick Milhorat, <strong>Koji Inoue</strong>, Tianyu Zhao, Tatsuya Kawahara, “Multimodal interaction with the autonomous android ERICA,” ICMI, pp.417-418, Nov. 2016. (Demo) [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LAL-ICMI16.pdf" target=blank>PDF</a>][<a href="http://dl.acm.org/citation.cfm?id=2998528" target=blank>Link</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong>, Pierrick Milhorat, Divesh Lala, Tianyu Zhao, Tatsuya Kawahara, “Talking with ERICA, an autonomous android,” SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pp.212-215, Sep. 2016. (Demo) [<a href="http://www.sigdial.org/workshops/conference17/proceedings/pdf/SIGDIAL25.pdf" target=blank>PDF</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Tatsuya Kawahara, Takashi Yamaguchi, <strong>Koji Inoue</strong>, Katsuya Takanashi, Nigel G. Ward, “Prediction and generation of backchannel form for attentive listening systems,” INTERSPEECH, pp.2980-2894, Sep. 2016. [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/KAW-INTERSP16.pdf" target=blank>PDF</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Takashi Yamaguchi, <strong>Koji Inoue</strong>, Koichiro YoshiNo. Katsuya Takanashi, Nigel G. Ward, Tatsuya Kawahara,  “Analysis and prediction of morphological patterns of backchannels for attentive listening agents,” International Workshop on Spoken Dialog Systems (IWSDS), Jan. 2016. [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/YAM-IWSDS16.pdf" target="_blank">PDF</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong>, Yukoh Wakabayashi, Hiromasa Yoshimoto, Katsuya Takanashi, and Tatsuya Kawahara, “Enhanced speaker diarization with detection of backchannels using eye-gaze information in poster conversations,” INTERSPEECH, pp.3086-3090, Sep. 2015. (Oral) [<a href="http://www.sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INO-INTERSP15.pdf" target="_blank">PDF</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Yukoh Wakabayashi, <strong>Koji Inoue</strong>, Hiromasa Yoshimoto, and Tatsuya Kawahara, “Speaker Diarization based on Audio-Visual Integration for Smart Posterboard,” APSIPA ASC, FP2-1-1116, Dec. 2014. (Oral)</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong>, Yukoh Wakabayashi, Hiromasa Yoshimoto, and Tatsuya Kawahara, “Speaker Diarization using Eye-gaze Information in Multi-party Conversations,” INTERSPEECH, pp.562-566, Sep. 2014. (Poster) [<a href="http://www.sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INO-INTERSP14.pdf" target="_blank">PDF</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Randy Gomez, <strong>Koji Inoue</strong>, Keisuke Nakamura, Takeshi Mizumoto, and Kazuhiro Nakadai, “Speech-based Human-Robot Interaction Robust to Acoustic Reflections in Real Environment,” International Conference on Intelligent Robots and Systems (IROS), pp. 1367-1373, 2014 .</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong>, Hironobu Saito, and Yoshimitsu Kuroki, “Local intensity compensation using sparse representation,” International Conference on Pattern Recognition (ICPR), No. TuAT2.4, pp. 951-954, Tsukuba, Japan, Nov. 2012. (Oral, Oral acceptance rate = 16.1%, 313/1941) [<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6460292" target="_blank">Link</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong> and Yoshimitsu Kuroki, “On a constraint condition of sparse representation for illumination-robust face recognition,” International Workshop on Image &amp; Signal Processing and Retrieval (IWISPR 2012), Oct. 2012. (Poster)</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Kohei Isechi, <strong>Koji Inoue</strong>, and Yoshimitsu Kuroki, “Video compression using sparse representation,” International Workshop on Image &amp; Signal Processing and Retrieval (IWISPR), Oct. 2012. (Poster)</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong>, Hironobu Saito, and Yoshimitsu Kuroki, “ Motion compensation using sparse representation,” 2012 International Workshop on Smart Info-Media Systems in Asia (SISA 2012), RS3-10, Sep. 2012. (Poster)</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong> and Yoshimitsu Kuroki, “On sparse representation for face recognition under illumination change,” International Workshop on Advanced Image Technology (IWAIT), pp. 662-667, Jan. 2012. (Poster)</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Shoma Eguchi, <strong>Koji Inoue</strong>, Yoshimitsu Kuroki, Masayuki Kurosaki, Yuhei Nagao, and Hiroshi Ochi, “On Parallel 2D-DWT of JPEG 2000 conformed to Digital Cinema Initiatives using GPGPU,” International Workshop on Advanced Image Technology (IWAIT), pp. 668-671, Jan. 2012. (Poster)</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong> and Yoshimitsu Kuroki, “Illumination-Robust Face Recognition via Sparse Representation,” Visual Communication and Image Processing (VCIP), O-10.2, Nov. 2011. (Oral) [<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=06115969" target="_blank">Link</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong> and Yoshimitsu Kuroki, “Face Recognition under Illumination Change using Sparse Representation,” International Workshop on Target Recognition and Tracking (IWTRT), Oct. 2011. (Poster)</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Shoma Eguchi, <strong>Koji Inoue</strong>, and Yoshimitsu Kuroki, “Real Time 2D-DWT of JPEG 2000 for Digital Cinema using Multiple GPUs,” International Workshop on Target Recognition and Tracking (IWTRT), Oct. 2011. (Poster)</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>Koji Inoue</strong>, Yoshimitsu Kuroki, Masayuki Kurosaki, Yuhei Nagao, Baiko Sai, and Hiroshi Ochi, “A parallel Computing using CUDA for the 2D-DWT of JPEG 2000,” International Symposium on Multimedia and Communication Technology (ISMAC), TS1-5, Sep. 2011. (Oral)</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Kenjiro Sugimoto, <strong>Koji Inoue</strong>, Yoshimitsu Kuroki, and Sei-ichiro Kamata, “A color distribution descriptor for medicine package recognition,” China-Japan-Korea Joint Workshop on Pattern Recognition (CJKPR), pp. 64-69, Nov. 2010. (Poster)</span></li>
</ul>
<hr />
<p><span style="font-size: 10pt;"><strong><a id="conference_j"></a>全国大会・研究会</strong></span></p>

<ul>
	<li><span style="font-size: 10pt;">
	彭子豪, 傅雅慧, Divesh Lala, 越智景子, <strong>井上昂治</strong>, 河原達也. <br>
	感情語りコーパスを用いたバリデーション応答の生成.<br>
	人工知能学会研究会資料, SLUD-099-23, 2023.<br>
	[<a href="https://www.jstage.jst.go.jp/article/jsaislud/99/0/99_119/_article/-char/ja/" target="_blank">Link</a>]<br>
	</span></li>
	</ul>

<ul>
	<li><span style="font-size: 10pt;">
	越智景子, <strong>井上昂治</strong>, Divesh Lala, 河原達也, 熊崎博一. <br>
	精神科デイケアのための傾聴対話システム：きくロボ.<br>
	人工知能学会研究会資料, SLUD-099-16, 2023.<br>
	[<a href="https://www.jstage.jst.go.jp/article/jsaislud/99/0/99_78/_article/-char/ja/" target="_blank">Link</a>]<br>
	</span></li>
	</ul>

<ul>
	<li><span style="font-size: 10pt;">
	川井悠生, 山本賢太, 越智景子, Lala Divesh, <strong>井上昂治</strong>, 河原達也. <br>
	半自律対話による同時並列傾聴システム.<br>
	人工知能学会研究会資料, SLUD-099-14, 2023.<br>
	[<a href="https://www.jstage.jst.go.jp/article/jsaislud/99/0/99_66/_article/-char/ja/" target="_blank">Link</a>]<br>
	</span>
	<strong><span style="color: #ff0000;">第14回対話システムシンポジウム若手優秀賞</span></strong>
</li>
	</ul>

<ul>
	<li><span style="font-size: 10pt;">
	<strong>井上昂治</strong>, Divesh Lala, 越智景子, 河原達也, Gabriel Skantze. <br>
	音声対話システムの客観評価のためのユーザのマルチモーダルなふるまいの分析.<br>
	人工知能学会研究会資料, SLUD-099-01, 2023.<br>
	[<a href="https://www.jstage.jst.go.jp/article/jsaislud/99/0/99_01/_article/-char/ja/" target="_blank">Link</a>]<br>
	</span></li>
	</ul>

<ul>
<li><span style="font-size: 10pt;">
Prediction of Validating Response from Emotional Storytelling Corpus.<br>
Zi Haur Pang, Yahui Fu, Divesh Lala, Keiko Ochi, Koji Inoue, Tatsuya Kawahara.<br>
第37回 人工知能学会全国大会, 2023. <br>
[<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2023/0/JSAI2023_2O5OS2a03/_pdf" target="_blank">PDF</a>]
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
越智景子, <strong>井上昂治</strong>, 河原達也, 大西祐美, 熊﨑博一.<br>
傾聴ロボットを用いた精神科デイケアでの会話の基礎検討.<br>
第41回 日本社会精神医学会, 2023. 
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
三村正人, <strong>井上昂治</strong>, 河原達也, 中村友彦, 猿渡洋.<br>
実環境下日本語話し言葉音声コーパスの構築と音声認識ベンチマーク.<br>
情報処理学会研究報告, SLP-146-12, 2023.<br>
[<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=224409&item_no=1&page_id=13&block_id=8" target="_blank">Link</a>]<br>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
山本賢太, <strong>井上昂治</strong>, 河原達也. <br>
音声対話システムによる雑談対話におけるキャラクタ表現に基づくユーザ適応の検討.<br>
人工知能学会研究会資料, SLUD-096-29, 2022.<br>
[<a href="https://www.jstage.jst.go.jp/article/jsaislud/96/0/96_29/_article/-char/ja/" target="_blank">Link</a>]<br>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
越智景子, <strong>井上昂治</strong>, Lala Divesh, 河原達也.<br>
傾聴対話におけるユーザ発話に同調する相槌の韻律制御.<br>
日本音響学会2022秋季研究発表会, 3-8-9, 2022. 
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
有岡無敵, 山本賢太, <strong>井上昂治</strong>, 河原達也, 中村哲, 吉野幸一郎.<br>
遠隔操作アンドロイドを用いたマルチモーダル説得対話コーパスの収集と分析.<br>
言語処理学会第28回年次大会, B2-2, 2022.
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
川井悠生, 山本賢太, Lala Divesh, <strong>井上昂治</strong>, 河原達也.<br>
回答評価とキーワード抽出を用いた同時並列就職面接対話システム.<br>
情報処理学会 第84回全国大会, 2R-05, 2022.<br>
<strong><span style="color: #ff0000;">学生奨励賞</span></strong>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
長連成, <strong>井上昂治</strong>, 越智景子, 河原達也.<br>
大規模テキストデータを用いた事前学習による音声対話の相槌予測.<br>
情報処理学会 第84回全国大会, 2R-04, 2022.
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
村木優介, 山本賢太, Lala Divesh, <strong>井上昂治</strong>, 河原達也.<br>
半自律並列プレゼン対話システムのための質問分類による介入タイミング制御.<br>
情報処理学会 第84回全国大会, 2R-03, 2022.
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
<strong>井上昂治</strong>, Lala Divesh, 河原達也. <br>
共感を表出する音声対話システムのための共有笑い生成.<br>
人工知能学会研究会資料, SLUD-093-27, 2021.<br>
[<a href="https://www.jstage.jst.go.jp/article/jsaislud/93/0/93_155/_article/-char/ja/" target="_blank">Link</a>]<br>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
山本賢太, <strong>井上昂治</strong>, 河原達也. <br>
音声対話システムのユーザ適応に向けたパーソナリティの関係性の分析.<br>
人工知能学会研究会資料, SLUD-093-02, 2021.<br>
[<a href="https://www.jstage.jst.go.jp/article/jsaislud/93/0/93_09/_article/-char/ja/" target="_blank">Link</a>]<br>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
酒井和紀, 吉川雄一郎, <strong>井上昂治</strong>, 河原達也, 石黒浩. <br>
複数ロボットによる対話継続効果検証のための商業施設でのフィールド実験.<br>
第39回日本ロボット学会学術講演会, 2B4-02, 2021．<br>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
<strong>井上昂治</strong>.<br>
アンドロイドERICAの音声対話システム～マルチモーダルチューリングテストへの挑戦～.<br>
電子情報通信学会技術研究報告, SP2021-7, 2021. 
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
坂本寛弥, 山本賢太, Lala Divesh, <strong>井上昂治</strong>, 河原達也.<br>
話し手以外のユーザへの参与促進発話を生成する多人数傾聴対話システム.<br>
情報処理学会全国大会講演論文集, 6N-06, 2021.
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
<strong>井上昂治</strong>, Lala Divesh, 河原達也.<br>
ヒューマンロボットインタラクションのための相槌・笑いのリアルタイム検出.<br>
日本音響学会2021春季研究発表会, 1-2-9, 2021. 
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
<strong>井上昂治</strong>, Lala Divesh, 河原達也.<br>
ヒューマンロボットインタラクションにおける音響特徴に基づく共有笑いの予測.<br>
日本音響学会2021春季研究発表会, 1-2-8, 2021. 
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
山本賢太, <strong>井上昂治</strong>, 河原達也. <br>
VAEを用いた半教師あり学習による音声対話システムのためのキャラクタ表現.<br>
人工知能学会研究会資料, SLUD-C002-31, 2020.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=10940" target="_blank">Link</a>]<br>
<strong><span style="color: #ff0000;">第11回対話システムシンポジウム若手優秀賞</span></strong>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
田中滉己, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也. <br>
多様なふるまいからの好感判定に基づき発話構成要素を選択するお見合い対話システム.<br>
人工知能学会研究会資料, SLUD-C002-28, 2020.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=10937" target="_blank">Link</a>]
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
<strong>井上昂治</strong>, Lala Divesh, 山本賢太, 中村静, 高梨克也, 河原達也. <br>
アンドロイドERICAの傾聴対話システムにおけるWOZとの比較評価.<br>
人工知能学会研究会資料, SLUD-C002-21, 2020.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=10929" target="_blank">Link</a>]
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
<strong>井上昂治</strong>, Lala Divesh, 山本賢太, 中村静, 高梨克也, 河原達也.<br>
WOZとの比較による自律型アンドロイドERICAの傾聴対話システムの評価.<br>
日本音響学会2020秋季研究発表会, 3-2-7, 2020. 
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
<strong>井上昂治</strong>, 原康平, Lala Divesh, 山本賢太, 中村静, 高梨克也, 河原達也.<br>
自律型アンドロイドERICAによる就職面接対話.<br>
日本音響学会2020春季研究発表会, 3-4-3, 2020. 
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
磯西爽太, <strong>井上昂治</strong>, 高梨克也, 河原達也. <br>
質問タイプの分類に基づく登録外質問に対する応答生成.<br>
人工知能学会研究会資料, SLUD-B902-06, 2019.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=10546" target="_blank">Link</a>]
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
<strong>井上昂治</strong>, Divesh Lala, 山本賢太, 中村静, 高梨克也, 河原達也. <br>
自律型アンドロイドERICAによる傾聴対話システムの評価.<br>
人工知能学会研究会資料, SLUD-B902-04, 2019.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=10544" target="_blank">Link</a>]
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
原康平, <strong>井上昂治</strong>, Divesh Lala, 山本賢太, 中村静, 高梨克也, 河原達也. <br>
アンドロイドERICAによる面接対話における掘り下げ質問生成.<br>
人工知能学会研究会資料, SLUD-B902-03, 2019.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=10543" target="_blank">Link</a>]
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
春日悠生, <strong>井上昂治</strong>, 山本賢太, 高梨克也, 河原達也. <br>
ヒューマンロボットインタラクションコーパスへの焦点アノテーションの基準と予備的分析.<br>
人工知能学会研究会資料, SLUD-B901-03, 2019.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&page_id=13&active_action=repository_view_main_item_detail&item_id=10353&item_no=1&block_id=23" target="_blank">Link</a>]<br>
<strong><span style="color: #ff0000;">人工知能学会 2019年度研究会優秀賞</span></strong>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
<strong>井上昂治</strong>, Lala Divesh, 山本賢太, 中村静, 高梨克也, 河原達也.<br>
自律型アンドロイドERICAによる傾聴対話の評価.<br>
日本音響学会2019秋季研究発表会, 1-3-2, 2019. <br>
<strong><span style="color: #ff0000;">日本音響学会粟屋潔学術奨励賞</span></strong>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
山本賢太, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也. <br>
対話のふるまい制御によるキャラクタ表現モデルと対話コーパスによる検証.<br>
情報処理学会全国大会講演論文集, 2T-07, 2019.
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
田中滉己, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也. <br>
対話相手への好感に基づく発話構成要素の選択とお見合い対話システムへの実装.<br>
情報処理学会全国大会講演論文集, 2T-06, 2019.<br>
<strong><span style="color: #ff0000;">大会優秀賞</span></strong>, <strong><span style="color: #ff0000;">学生奨励賞</span></strong>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
磯西爽太, 田中滉己, <strong>井上昂治</strong>,  高梨克也, 河原達也. <br>
質問タイプの分類に基づく用例なし質問に対する応答生成.<br>
情報処理学会全国大会講演論文集, 2T-05, 2019.
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
原康平, <strong>井上昂治</strong>,  高梨克也, 河原達也. <br>
移行適格場の予測に基づくターンテイキング予測.<br>
情報処理学会全国大会講演論文集, 2T-04, 2019.
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
原康平, <strong>井上昂治</strong>, 高梨克也, 河原達也. <br>
移行適格場の情報を考慮したターンテイキング予測.<br>
人工知能学会研究会資料, SLUD-B803-10, 2019.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9710&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
山本賢太, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也.<br>
対話のふるまいに基づくキャラクタ表現の対話コーパスにおける分析.<br>
人工知能学会研究会資料, SLUD-B803-08, 2019.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9708&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]<br>
<strong><span style="color: #ff0000;">人工知能学会 2018年度研究会優秀賞</span></strong>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
<strong>井上昂治</strong>, Divesh Lala, 原康平, 相原邦光, 中村静, 高梨克也, 河原達也. <br>
自律型アンドロイドERICAによる就職面接対話.<br>
人工知能学会研究会資料, SLUD-B802-14, 2018.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9621&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
山本賢太, <strong>井上昂治</strong>, Divesh Lala, 中村静, 高梨克也, 河原達也. <br>
自律型アンドロイドERICAによる傾聴対話.<br>
人工知能学会研究会資料, SLUD-B802-13, 2018.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9620&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
田中滉己, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也. <br>
初対面対話における好感のモデリングと発話構成要素の選択.<br>
人工知能学会研究会資料, SLUD-B802-03, 2018.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9609&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
原康平, <strong>井上昂治</strong>, 高梨克也, 河原達也. <br>
相槌・フィラー予測とのマルチタスク学習によるターンテイキング予測.<br>
人工知能学会研究会資料, SLUD-B802-01, 2018.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9607&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
原康平, <strong>井上昂治</strong>, 高梨克也, 河原達也.<br>
相槌・フィラー予測とのマルチタスク学習による円滑なターンテイキング.<br>
第80回情報処理学会全国大会, 6Q-07, 2018. 
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
山本賢太, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也.<br>
自律型アンドロイドのキャラクタ表現のための対話の振る舞い制御モデルの構築と評価.<br>
第80回情報処理学会全国大会, 6Q-06, 2018. <br>
<strong><span style="color: #ff0000;">学生奨励賞</span></strong>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
田中滉己, <strong>井上昂治</strong>, 高梨克也, 河原達也.<br>
初対面対話における好感の生成と発話構成要素の予測のモデル.<br>
第80回情報処理学会全国大会, 6Q-05, 2018. <br>
<strong><span style="color: #ff0000;">学生奨励賞</span></strong>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
石田真也, <strong>井上昂治</strong>, 高梨克也, 河原達也.<br>
共感・発話促進のための多様な聞き手応答を生成する傾聴対話システム.<br>
第80回情報処理学会全国大会, 6Q-04, 2018. 
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
<strong>井上昂治</strong>, Lala Divesh, 高梨克也, 河原達也.<br>
自律型アンドロイドERICAにおけるエンゲージメント推定に基づく音声対話システム.<br>
日本音響学会2018春季研究発表会, 2-8-9, 2018. 
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
石田真也, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也. <br>
共感表出と発話促進のための聞き手応答を生成する傾聴対話システム.<br>
人工知能学会研究会資料, SLUD-B509-2, 2018.<br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9072&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">
<strong>井上昂治</strong>, Lala Divesh, Milhorat Pierrick, 高梨克也, 河原達也.<br>
潜在キャラクタモデルによるリアルタイム対話エンゲージメント推定.<br>
人工知能学会研究会資料, SLUD-B508-18, 2017. <br>
[<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=8937&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]<br>
<strong><span style="color: #ff0000;">若手奨励賞</span></strong>, <strong><span style="color: #ff0000;">人工知能学会 2017年度研究会優秀賞</span></strong>
</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, Lala Divesh, Milhorat Pierrick, 石田真也, 趙天雨, 高梨克也, 河原達也, &#8220;自律型アンドロイドERICAにおける多様な聞き手応答を用いた傾聴対話,&#8221; 人工知能学会研究会資料, SLUD-B508-11, 2017. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=8928&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">山本賢太, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;自律型アンドロイドの対話の振る舞い制御モデルによる キャラクタ表現法の検討,&#8221; 人工知能学会研究会資料, SLUD-B508-05, 2017. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=8921&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, Lala Divesh, 吉井和佳, 高梨克也, 河原達也, &#8220;潜在キャラクタモデルによる聞き手のふるまいに基づく対話エンゲージメントの推定,&#8221; 日本音響学会2017秋季研究発表会, 2-Q-12, 2017. </span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">稲熊寛文, <strong>井上昂治</strong>, 三村正人, 河原達也, &#8220;End-to-endモデルによる音声対話中のsocial signalsの検出,&#8221; 日本音響学会2017秋季研究発表会, 1-10-16, 2017. <strong><span style="color: #ff0000;">学生優秀発表賞</span></strong></span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">稲熊寛文, <strong>井上昂治</strong>, 三村正人, 河原達也, &#8220;End-to-endモデルによるsocial signals検出および音声認識との統合,&#8221; 情報処理学会研究報告, SLP-117-7, 2017. </span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">勝見久央, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;自律型アンドロイドによる対話における同調的笑いの生成,&#8221; 情報処理学会研究報告, SLP-116-4, 2017. <strong><span style="color: #ff0000;">学生奨励賞</span></strong></span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">稲熊寛文, <strong>井上昂治</strong>, 河原達也, &#8220;ニューラルネットによる音声対話における非言語的振る舞いの検出,&#8221; 第79回情報処理学会全国大会, 7M-04, 2017. <strong><span style="color: #ff0000;">学生奨励賞</span></strong></span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">勝見久央, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;自律型アンドロイドによる対話における「同調的笑い」の生成,&#8221; 第79回情報処理学会全国大会, 7M-03, 2017. </span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">石田真也, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;傾聴対話システムにおける自分語りを含む多様な聞き手応答の生成,&#8221; 第79回情報処理学会全国大会, 7M-02, 2017. </span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">山本賢太, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;自律型アンドロイドのキャラクタ表現のための対話の振る舞い制御,&#8221; 第79回情報処理学会全国大会, 7M-01, 2017. </span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, Lala Divesh, 高梨克也, 河原達也, &#8220;聞き手の多様なふるまいに基づく対話エンゲージメントの推定,&#8221; 日本音響学会2017春季研究発表会, 3-5-1, 2017. </span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 三村正人, 石井カルロス寿憲, 坂井信輔, 河原達也, &#8220;DAEを用いたリアルタイム遠隔音声認識,&#8221; 日本音響学会2017春季研究発表会, 1-Q-6, 2017. </span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">李遠超, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;ヒューマンロボットインタラクションにおける韻律とテキスト情報を組み合わせた感情認識と評価応答選択,&#8221; 人工知能学会研究会資料, SLUD-B505-09, 2017. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=8547&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">中西亮輔, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;円滑な発話権制御のための談話行為の連鎖に基づく フィラーの生起と形態の予測,&#8221; 人工知能学会研究会資料, SLUD-B506-04, 2017. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=8542&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">稲熊寛文, <strong>井上昂治</strong>, 三村正人, 河原達也, &#8220;LSTM-CTCによる音声対話におけるSocial Signalsの検出,&#8221;  情報処理学会研究報告, SLP-115-9, 2017. [<a href="https://webcache.googleusercontent.com/search?q=cache:f9M9SWZsyx8J:https://ipsj.ixsq.nii.ac.jp/ej/%3Faction%3Drepository_uri%26item_id%3D177382+&cd=1&hl=ja&ct=clnk&gl=jp" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">中西亮輔, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;発話行為の連鎖を考慮したフィラーの生起と形態の分析,&#8221; 人工知能学会研究会資料, SLUD-B505-30, 2016. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=1310&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">稲熊寛文, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;初対面対話における韻律的特徴に基づくアイスブレーキングの分析と予測,&#8221; 人工知能学会研究会資料, SLUD-B505-29, 2016. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=1309&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, Lala Divesh, 高梨克也, 河原達也, &#8220;階層ベイズモデルを用いた聞き手の多様なふるまいに基づく対話エンゲージメントの推定,&#8221; 人工知能学会研究会資料, SLUD-B505-28, 2016. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=1308&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, Milohorat Pierrick, Lala Divesh, 趙天雨, 河原達也, &#8220;自律型アンドロイドERICAによる社会的役割に則したインタラクション,&#8221; 人工知能学会研究会資料, SLUD-B505-7, 2016. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=1288&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">石田真也, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;傾聴対話システムのための発話を促す聞き手応答の生成,&#8221; 人工知能学会研究会資料, SLUD-B504-01, 2016. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=1147&item_no=1&page_id=13&block_id=23" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">稲熊寛文, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;初対面対話における場の和みのマルチモーダルな分析と検出,&#8221; 第78回情報処理学会全国大会, 6Q-02, 2016. </span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">石田真也, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;傾聴対話システムのための多様な聞き手応答の生成,&#8221; 第78回情報処理学会全国大会, 6Q-01, 2016. <strong><span style="color: #ff0000;">学生奨励賞</span></strong></span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 三村正人, 石井カルロス寿憲, 河原達也, &#8220;自律型アンドロイドERICAのための遠隔音声認識,&#8221; 日本音響学会2016春季研究発表会, 1-1-1, 2016. <strong><span style="color: #ff0000;">学生優秀発表賞</span></strong></span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">中西亮輔, <strong>井上昂治</strong>, 中村静, 高梨克也, 河原達也, &#8220;自律型アンドロイドによる円滑な発話権制御のためのフィラーの生起位置と形態の分析,&#8221; 人工知能学会研究会資料, SLUD-B503-11, 2016.</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">山口貴史, <strong>井上昂治</strong>, 吉野幸一郎, 高梨克也, Ward G. Nigel, 河原達也, &#8220;傾聴対話システムのための言語情報と韻律情報に基づく多様な形態の相槌の生成,&#8221; 人工知能学会研究会資料, SLUD-B503-9, 2016.</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 河原達也, &#8220;自律型アンドロイドEricaのための音声対話システム,&#8221; 人工知能学会研究会資料, SLUD-B502-5, 2015. [<a href="http://sap.ist.i.kyoto-u.ac.jp/erato/INO-slud15-10.pdf" target="_blank">PDF</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">山口貴史, <strong>井上昂治</strong>, 吉野幸一郎, 高梨克也, Ward G. Nigel, 河原達也, &#8220;多様な相槌をうつ傾聴対話システムのための相槌形態の予測,&#8221; 人工知能学会研究会資料, SLUD-B502-1, 2015.</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 若林佑幸, 吉本廣雅, 高梨克也, 河原達也, &#8220;ポスター会話における音響・視線情報の確率的統合による 話者区間及び相槌の検出,&#8221; 日本音響学会2015秋季研究発表会, 2-2-4, 2015.</span></li>
</ul>
<ul>
<li id="aeaoofnhgocdbnbeljkmbjdmhbcokfdb-mousedown"><span style="font-size: 10pt;"><strong>井上昂治</strong>, 若林佑幸, 吉本廣雅, 高梨克也, 河原達也, &#8220;スマートポスターボードにおける視線情報を用いた話者区間及び相槌の検出,&#8221; 情報処理学会研究報告, MUS-107-68, 2015. [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=142054&item_no=1&page_id=13&block_id=8" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li id="aeaoofnhgocdbnbeljkmbjdmhbcokfdb-mousedown"><span style="font-size: 10pt;"><strong>井上昂治</strong>, 若林佑幸, 吉本廣雅, 高梨克也, 河原達也, &#8220;スマートポスターボードにおける視線情報を用いた話者区間検出及び相槌の同定,&#8221; 第77回情報処理学会全国大会, 6P-09, 2015. <strong><span style="color: #ff0000;">学生奨励賞</span></strong></span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">若林佑幸, 中山雅人, 西浦敬信, 山下洋一, <strong>井上昂治</strong>, 吉本廣雅, 河原達也, &#8220;拡散性雑音環境下における多人数会話のマルチモーダル話者区間検出.&#8221; 日本音響学会2015春季研究発表会, 1-Q-24, 2015.</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">山口貴史, <strong>井上昂治</strong>, 吉野幸一郎, 高梨克也, 河原達也, &#8220;傾聴対話における相槌形態と先行発話の統語構造の関係の分析,&#8221; 人工知能学会研究会資料, SLUD-B403-4, 2015.</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 若林佑幸, 吉本廣雅, 高梨克也, 河原達也, &#8220;ポスター会話における音響・視線情報を統合した話者区間及び相槌の検出,&#8221; 情報処理学会研究報告, SLP-105-9, 2015. [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=113132&amp;item_no=1&amp;page_id=13&amp;block_id=8" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 若林佑幸, 吉本廣雅, 河原達也, “多人数会話における音響情報と視線情報の確率的統合による話者区間検出,” 日本音響学会2014秋季研究発表会, 2-8-4, 2014.</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 若林佑幸, 吉本廣雅, 河原達也, “多人数会話における視線情報を用いた話者区間検出,” 情報処理学会研究報告, SLP-102-1, 2014. [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=102192&amp;item_no=1&amp;page_id=13&amp;block_id=8" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">若林佑幸, <strong>井上昂治</strong>, 河原達也, 中井駿介, 宮崎亮一, 猿渡洋, “スマートポスターボードにおける音響情報と画像情報の統合による話者区間検出,” 日本音響学会2014春季研究発表会, 2-Q4-7, 2014.</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">中井駿介，宮崎亮一，猿渡洋，中村哲，<strong>井上昂治</strong>，若林佑幸，河原達也, “スマートポスターボードにおける実環境を想定した複数話者分離,” 日本音響学会2014春季研究発表会, 2-Q4-8, 2014.</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 松隈俊大, 黒木祥光, “相互錐制約部分空間法,” 第15回画像の認識・理解シンポジウム (MIRU 2012), OS11-05, Aug. 2012. (Oral, Oral acceptance rate = 37.2%, 48/129) [<a href="http://www.ieice.or.jp/iss/prmu/jpn/miru/archive/MIRU2012/pdf/OS11-05.pdf" target="_blank">Link</a>]</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">黒崎正行, 伊東亮, 松尾宗明, 宮岡佑弥, <strong>井上昂治</strong>, 江口翔馬, 尾知博, 黒木祥光,  宮崎明雄, “暗号領域での認証を用いたJPEG 2000 画像無線伝送システム,” 電子情報通信学会 2012 年総合大会, March 2012.</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 黒木祥光,  “スパース表現を用いた照明変化に頑健な顔認識に関する研究,” 平成23年度(第64回)電気関係学会九州支部連合大会, Sep. 2011. (Oral)</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;">江口翔馬, <strong>井上昂治</strong>,  黒木祥光,  “ディジタルシネマの実時間処理に向けたGPGPUによるJPEG2000の高速化,” 平成23年度(第64回)電気関係学会九州支部連合大会, Sep. 2011.</span></li>
</ul>
<ul>
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 江口翔馬, 黒木祥光, 黒崎正行, 尾知博, “複数のGPUを用いたデジタルシネマ画像の実時間ウェーブレット変換,” 電子情報通信学会技術研究報告, SIS-111-210, pp. 45-49, Sep. 2011. [<a href="http://ci.nii.ac.jp/naid/110008899640" target="_blank">Link</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">松尾正輝, <strong>井上昂治</strong>, 黒崎正行, 黒木祥光, 斉培恒, 尾知博, “4K デジタルシネマ無線伝送システムのための JPEG 2000 並列化,” 画像電子学会 第255回研究会, March 2011.</span></li>
</ul>
<hr />

<p><span style="font-size: 10pt;"><b><a id="kaisetsu"></a>学会誌</b></span></p>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;">
		山本祐輔, 山野泰子, <strong>井上昂治</strong>, 榊剛史, 岩澤駿.<br>
		表紙企画 人工知能をめぐる旅: JSAI 研究会の窓 ④—先進的学習科学と工学研究会（ALST）—<br>
		人工知能学会誌, Vol.39, No.4, pp.577-579, 2024.<br>
		[<a href="https://www.jstage.jst.go.jp/article/jjsai/39/4/39_577/_article/-char/ja/" target="_blank">Link</a>]
	</li>
</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;">
		山野泰子, <strong>井上昂治</strong>, 榊剛史, 山本祐輔, 岩澤駿.<br>
		表紙企画 人工知能をめぐる旅: JSAI 研究会の窓 ③— 知識ベースシステム研究会（KBS）—<br>
		人工知能学会誌, Vol.39, No.3, pp.435-437, 2024.<br>
		[<a href="https://www.jstage.jst.go.jp/article/jjsai/39/3/39_435/_article/-char/ja" target="_blank">Link</a>]
	</li>
</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;">
		榊剛史, 山野泰子, <strong>井上昂治</strong>, 山本祐輔, 岩澤駿.<br>
		表紙企画 人工知能をめぐる旅: JSAI 研究会の窓 ②—人工知能基本問題研究会 (FPAI)—<br>
		人工知能学会誌, Vol.39, No.2, pp.271-273, 2024.<br>
		[<a href="https://www.jstage.jst.go.jp/article/jjsai/39/2/39_271/_article/-char/ja/" target="_blank">Link</a>]
	</li>
</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;">
		<strong>井上昂治</strong>, 山野泰子, 榊剛史, 山本祐輔, 岩澤駿.<br>
		表紙企画 人工知能をめぐる旅: JSAI 研究会の窓 ①—言語・音声理解と対話処理研究会 (SLUD)—<br>
		人工知能学会誌, Vol.39, No.1, pp.86-89, 2024.<br>
		[<a href="https://www.jstage.jst.go.jp/article/jjsai/39/1/39_86/_article/-char/ja/" target="_blank">Link</a>]
	</li>
</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;">
		<strong>井上昂治</strong>.<br>
		特集：編集委員 今年の抱負2024「学会誌に表紙は必要か？」<br>
		人工知能学会誌, Vol.39, No.1, pp.11, 2024.<br>
		[<a href="https://www.jstage.jst.go.jp/article/jjsai/39/1/39_11/_article/-char/ja/" target="_blank">Link</a>]
	</li>
</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;">榊剛史, <strong>井上昂治</strong>, 山野泰子, 鳥海不二夫, 岩澤駿, 松原仁, 杉本舞, 谷口忠大.<br>
		表紙企画 座談会：人工知能歴史絵巻の完成に際して<br>
	人工知能学会誌, Vol.38, No.6, pp.980-989, 2023.<br>
	[<a href="https://www.jstage.jst.go.jp/article/jjsai/38/6/38_980/_article/-char/ja" target="_blank">Link</a>]</li>
	</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;">山野泰子, 榊剛史, <strong>井上昂治</strong>, 山本祐輔, 岩澤駿, 松原仁, 杉本舞, 谷口忠大.<br>
		アーティクル：表紙企画 人工知能歴史絵巻：これまでのAI これからのAI ⑥ — 研究者が考える未来のAI：道は行くことでつくられる—<br>
		人工知能学会誌, Vol.38, No.6, pp.975-979, 2023.<br>
		[<a href="https://www.jstage.jst.go.jp/article/jjsai/38/6/38_975/_article/-char/ja" target="_blank">Link</a>]</li>
	</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 榊剛史, 山野泰子, 山本祐輔, 岩澤駿, 松原仁, 杉本舞, 谷口忠大.<br>
	アーティクル：表紙企画 人工知能歴史絵巻：これまでのAI これからのAI ⑤ — そして生成AI ブームへAll You Need Is [MASK] —<br>
	人工知能学会誌, Vol.38, No.5, pp.771-774, 2023.<br>
	[<a href="https://www.jstage.jst.go.jp/article/jjsai/38/5/38_771/_article/-char/ja/" target="_blank">Link</a>]</li>
	</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;"><strong>井上昂治</strong>.<br>
	どうする対話研究<br>
	日本バーチャルリアリティ学会誌, Vol.28, No.2, pp.33-34, 2023.<br>
	[<a href="https://www.jstage.jst.go.jp/article/jvrsj/28/2/28_33/_article/-char/ja" target="_blank">Link</a>]</li>
	</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;"><strong>井上昂治</strong>.<br>
	書評：セルジュ・ティスロン 著, 阿部又一郎 訳：ロボットに愛される日―AI 時代のメンタルヘルス<br>
	人工知能学会誌, Vol.38, No.4, pp.593, 2023.<br>
	[<a href="https://www.jstage.jst.go.jp/article/jjsai/38/4/38_593/_article/-char/ja/" target="_blank">Link</a>]</li>
	</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;">榊剛史, <strong>井上昂治</strong>, 山野泰子, 岩澤駿, 松原仁, 杉本舞, 谷口忠大.<br>
	アーティクル：表紙企画 人工知能歴史絵巻：これまでのAI これからのAI ④ ― ブームがやって来るヤァ！ヤァ！ヤァ！―<br>
	人工知能学会誌, Vol.38, No.4, pp.595-598, 2023.<br>
	[<a href="https://www.jstage.jst.go.jp/article/jjsai/38/4/38_595/_article/-char/ja/" target="_blank">Link</a>]</li>
	</ul>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;">山野泰子, <strong>井上昂治</strong>, 榊剛史, 岩澤駿, 松原仁, 杉本舞, 谷口忠大.<br>
	アーティクル：表紙企画 人工知能歴史絵巻：これまでのAI これからのAI ③ —二度目の冬の蠢き—<br>
	人工知能学会誌, Vol.38, No.3, pp.440-443, 2023.<br>
	[<a href="https://www.jstage.jst.go.jp/article/jjsai/38/3/38_440/_article/-char/ja/" target="_blank">Link</a>]</li>
	</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 山野泰子, 榊剛史, 岩澤駿, 松原仁, 杉本舞, 谷口忠大.<br>
アーティクル：表紙企画 人工知能歴史絵巻：これまでのAI これからのAI ② —冬の時代から第二次ブームへ—<br>
人工知能学会誌, Vol.38, No.2, pp.298-301, 2023.<br>
[<a href="https://www.jstage.jst.go.jp/article/jjsai/38/2/38_298/_article/-char/ja/" target="_blank">Link</a>]</li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">榊剛史,<strong>井上昂治</strong>, 山野泰子, 岩澤駿, 杉本舞, 松原仁, 谷口忠大.<br>
アーティクル：表紙企画 人工知能歴史絵巻：これまでのAI これからのAI —第三次AIブームを振り返って—<br>
人工知能学会誌, Vol.38, No.1, pp.92-95, 2023.<br>
[<a href="https://www.jstage.jst.go.jp/article/jjsai/38/1/38_92/_article/-char/ja/" target="_blank">Link</a>]</li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">佐久間洋司, <strong>井上昂治</strong>.<br>
人類の分断を克服し調和を実現するための科学技術に関する調査研究<br>
人工知能学会誌, Vol.36, No.6, pp.702-709, 2021.<br>
[<a href="https://www.jstage.jst.go.jp/article/jjsai/36/6/36_702/_article/-char/ja" target="_blank">Link</a>]</li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 岡田将吾.<br>
特集：「若手研究者による2050 年の未来予測～ムーンショット型研究開発 ミレニア・プログラムより～」にあたって<br>
人工知能学会誌, Vol.36, No.6, pp.672-673, 2021.<br>
[<a href="https://www.jstage.jst.go.jp/article/jjsai/36/6/36_672/_article/-char/ja" target="_blank">Link</a>]</li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>.<br>
50年経っても読まれる論文を.<br>
人工知能学会誌, Vol.36, No.3, pp.330, 2021.<br>
[<a href="https://www.jstage.jst.go.jp/article/jjsai/36/3/36_330/_article/-char/ja" target="_blank">Link</a>]</li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 河原達也.<br>
アンドロイドを用いた音声対話研究<br>
日本音響学会誌, Vol.76, No.4, pp.236-243, 2020.<br>
[<a href="https://www.jstage.jst.go.jp/article/jasj/76/4/76_236/_article/-char/ja/" target="_blank">Link</a>]</li>
</ul>

<hr />

<p><span style="font-size: 10pt;"><b><a id="other"></a>その他</b></span></p>
<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;">Yahui Fu, <strong>井上昂治</strong>, Chenhui Chu, 河原達也.<br>
	Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation.<br>
	第18回京都大学ICTイノベーション, Feb. 2024. <br>
	[<a href="https://ict-nw.i.kyoto-u.ac.jp/ict-innovation/18th/panel/1907/" target="_blank">Link</a>] <strong><span style="color: #ff0000;">優秀研究賞</span></strong></li>
	</ul>

<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>.<br>
人を支える・人に共感する・人を超える対話システム.<br>
日本音響学会 学生・若手フォーラム ビギナーズセミナー in KANSAI ～すべての道は音声対話に通ず～(第33回関西支部談話会), March 2023.<br>
[<a href="http://asj-fresh.acoustics.jp/event/2023-02-2965" target="_blank">Link</a>]</li>
</ul>

<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">Tatsuya Kawahara, <strong>Koji Inoue</strong>, Divesh Lala.<br>
Intelligent Conversational Android ERICA Applied to Attentive Listening and Job Interview<br>
arXiv:2105.00403, 2021.<br>
[<a href="https://arxiv.org/abs/2105.00403" target="_blank">Link</a>]</li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>.<br>
コロナ禍における多人数音声対話システムの被験者実験.<br>
電子情報通信学会 VNV研究会 第15回年次大会, March 2021.<br>
</ul>

<ul style="list-style-type: disc;">
	<li><span style="font-size: 10pt;"><strong>井上昂治</strong>.<br>
	アンドロイドと音声対話システムの融合 -深層ヒューマンロボットインタラクションの実現に向けて-.<br>
	日本ロボット学会ヒューロビント研究専門委員会 若手ロボティクス研究会, Jan. 2021.<br>
</ul>

<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">有本庸浩, 佐藤志貴, <strong>井上昂治</strong>, 山本賢太, 赤間怜奈.<br>
国際会議報告（SIGDIAL, ACL, ICMI, Interspeech, EMNLP）.<br>
人工知能学会 言語・音声理解と対話処理研究会 (SLUD) 第90回研究会 (第11回対話システムシンポジウム), Dec. 2020.<br>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>.<br>
アンドロイドERICAによる傾聴対話システム.<br>
第４回 JST ERATO石黒共生HRIプロジェクトシンポジウム, Aug. 2020.<br>
[<a href="https://sites.google.com/irl.sys.es.osaka-u.ac.jp/shri-symposium-2020/" target="_blank">Link</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">山本賢太, <strong>井上昂治</strong>, 河原達也.<br>
多様な応答を生成する傾聴対話システム -ロボットがあなたの話を聴きます-.<br>
第14回京都大学ICTイノベーション, Feb. 2020. <br>
[<a href="http://ict-nw.i.kyoto-u.ac.jp/ict-innovation/14th/panel/panel.php?id=19" target="_blank">Link</a>] <strong><span style="color: #ff0000;">優秀研究賞</span></strong></li>
</ul>
<ul style="list-style-type: disc;">
<li><strong>井上昂治</strong>.<br>
博士の学位取得後の大学に就職するキャリア.<br>
IEEE 関西支部 Young Professionals 博士課程のキャリアについて語る会, Sep. 2019.<br>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">石井亮, <strong>井上昂治</strong>, 千葉祐弥, 角森唯子, 成松宏美, 福田悠人, 増村亮.<br>
国際会議報告（SIGDIAL, ACL, IJCAI-ECAI, COLING, Interspeech, ICMI, EMNLP, IVA, SEMDIAL）.<br>
人工知能学会 言語・音声理解と対話処理研究会 (SLUD) 第84回研究会 (第9回対話システムシンポジウム), Nov. 2018.<br>
[<a href="https://github.com/jsai-slud/sig-slud/raw/master/material/84th/DSS9_conference_reports.pdf" target="_blank">PDF</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>.<br>
対話におけるエンゲージメント推定.<br>
第２回 JST ERATO石黒共生HRIプロジェクトシンポジウム, Aug. 2018.<br>
[<a href="https://sites.google.com/view/erato-shri-symposium2018/" target="_blank">Link</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 河原達也.<br>
会話エンゲージメントの自動認識 -相手が会話に興味があるかがわかります-.<br>
第12回京都大学ICTイノベーション, March 2018.<br>
[<a href="http://ict-nw.i.kyoto-u.ac.jp/ict-innovation/12th/panel/panel.php?id=10" target="_blank">Link</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, “聞き手のふるまいに着目した対話エンゲージメントの分析と予測,” 電子情報通信学会第52回VNV研究会 May. 2016.</span></li></ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, “対話ロボットは空気を読めるか？,” 日本音響学会北陸支部第7回音響セミナー ビギナー成果発表会, Dec. 2015.</span> [<a href="http://www.ais.jaist.ac.jp/asj-hokuriku/onkyo-seminar7th.html" target="_blank">Link</a>]</li></ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">山口貴史, <strong>井上昂治</strong>, 吉野幸一郎, 高梨克也, Nigel G. Ward, 河原達也, “多様な形態の相槌をうつ傾聴対話システム,” 第18回日本音響学会関西支部若手研究者交流研究発表会, 11, Dec. 2015.</span> <strong><span style="color: #ff0000;">優秀奨励賞</span></strong></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治, </strong>“ポスター会話における音響・視線情報を統合した話者区間及び相槌の検出,” 第41回関西音声合同ゼミ, A-10, July 2015. </span></li>
</ul>
<ul style="list-style-type: disc;">
<li><strong>Koji Inoue</strong>, “Multi-modal conversational analysis in poster sessions,” Multi-modal Interaction Workshop, April 2015. [<a href="http://www.ii.ist.i.kyoto-u.ac.jp/?p=5616" target="_blank">Link</a>]</li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 吉本廣雅, 河原達也, “スマートポスターボード -聴衆の反応のセンシング-,” 第9回京都大学ICTイノベーション, March 2015. [<a href="http://ict-nw.i.kyoto-u.ac.jp/ict-innovation/2015/panel/panel.php?id=1" target="_blank">Link</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治, </strong>若林佑幸, 吉本廣雅, 河原達也, “多人数会話における視線情報を用いた話者区間検出,” 第17回日本音響学会関西支部若手研究者交流研究発表会, 10, Dec. 2014. [<a href="http://asj-kansai.acoustics.jp/event/wakate_2014_report.html" target="_blank">Link</a>]　<strong><span style="color: #ff0000;">優秀奨励賞</span></strong></span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;">若林佑幸, 中山雅人, 西浦敬信, <strong>井上昂治</strong>, 吉本廣雅, 河原達也, “マルチモーダル話者ダイアライゼーション ～Who speaks when?～,” 第17回日本音響学会関西支部若手研究者交流研究発表会, 12, Dec. 2014.</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治, </strong>“多人数会話における視線情報を用いた話者区間検出,” 第39回関西音声合同ゼミ, B.3, July 2014. [<a href="http://www.aspl.is.ritsumei.ac.jp/kansai/2014/index.html#presen_program" target="_blank">Link</a>]</span></li>
</ul>
<ul style="list-style-type: disc;">
<li><span style="font-size: 10pt;"><strong>井上昂治</strong>, 河原達也, “スマートポスターボード -聴衆の反応のセンシング-,” 第8回京都大学ICTイノベーション, Dec. 2013.</span></li>
</ul>
					</div><!-- .entry-content -->
		<footer class="entry-meta">
					</footer><!-- .entry-meta -->
	</article><!-- #post -->
				
<div id="comments" class="comments-area">

	
	
			
</div><!-- #comments .comments-area -->			
		</div><!-- #content -->
	</div><!-- #primary -->


		</div><!-- #main .wrapper -->
	<footer id="colophon" role="contentinfo">
		<div class="site-info">
						<a href="http://ja.wordpress.org/" title="セマンティックなパブリッシングツール">Proudly powered by WordPress</a>
		</div><!-- .site-info -->
	</footer><!-- #colophon -->
</div><!-- #page -->

<script type='text/javascript' src='https://inokoj.github.io/wp-content/themes/twentytwelve/js/navigation.js?ver=20140711'></script>
</body>
</html>
