<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Publications | Koji Inoue</title>
    <!-- Tailwind CSS aCDN を読み込み -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Google Fonts (Inter) を読み込み -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700;800&family=Noto+Sans+JP:wght@400;500;700&display=swap" rel="stylesheet">
    <style>
        /* カスタムスタイル */
        html {
            scroll-behavior: smooth;
        }
        body {
            font-family: 'Inter', 'Noto Sans JP', sans-serif;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }
        .section-card {
            background-color: white;
            border-radius: 1.5rem; /* 24px */
            padding: 2.5rem; /* 40px */
            margin-bottom: 3rem; /* 48px */
            box-shadow: 0 20px 25px -5px rgb(0 0 0 / 0.1), 0 8px 10px -6px rgb(0 0 0 / 0.1);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        @media (max-width: 768px) {
            .section-card {
                padding: 1.5rem; /* 24px */
            }
        }
        .section-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 25px 50px -12px rgb(0 0 0 / 0.25);
        }
        .section-title {
            font-size: 2.25rem; /* 36px */
            font-weight: 800;
            margin-bottom: 2rem; /* 32px */
            padding-bottom: 1rem; /* 16px */
            border-bottom: 2px solid;
            border-image: linear-gradient(to right, #3b82f6, #9333ea) 1;
        }
        .subsection-title {
            font-size: 1.875rem; /* 30px */
            font-weight: 700;
            margin-top: 2.5rem; /* 40px */
            margin-bottom: 1.5rem; /* 24px */
            padding-bottom: 0.5rem; /* 8px */
            border-bottom: 1px solid #e2e8f0; /* slate-200 */
        }
        /* 箇条書きのスタイル */
        .publication-list li {
            padding-left: 1rem;
            text-indent: -1rem;
        }
        .publication-list li::before {
             content: '■';
             margin-right: 0.5rem;
             color: #3b82f6;
        }

    </style>
</head>
<body class="bg-slate-50 text-slate-700">

    <div class="container mx-auto max-w-4xl px-4 py-8 md:py-16">

        <header class="text-center mb-16">
            <h1 class="text-5xl font-extrabold text-slate-900">Koji Inoue (KI)</h1>
            <p class="text-2xl text-slate-500 mt-2">井上 昂治</p>
            <p class="text-lg text-slate-600 mt-4">Kyoto University / The University of Osaka, Japan</p>
        </header>

        <nav class="flex justify-center items-center gap-4 md:gap-8 mb-16 bg-white/70 backdrop-blur-lg p-4 rounded-full shadow-md sticky top-4 z-50">
            <a href="https://inokoj.github.io/" class="bg-blue-600 text-white px-5 py-2 rounded-full font-bold hover:bg-blue-700 transition-all duration-300 shadow-sm">Top</a>
            <a href="https://inokoj.github.io/profile.html" class="bg-blue-600 text-white px-5 py-2 rounded-full font-bold hover:bg-blue-700 transition-all duration-300 shadow-sm">Bio</a>
            <a href="https://inokoj.github.io/publications.html" class="bg-purple-600 text-white px-5 py-2 rounded-full font-bold hover:bg-purple-700 transition-all duration-300 shadow-sm">Publications</a>
        </nav>

        <main>
            <section id="publications-list" class="section-card">
                <h1 class="section-title">Publications</h1>

                <div class="flex flex-wrap gap-x-4 gap-y-2 mb-8 text-blue-600">
                    <a href="#journal" class="hover:underline">Journal papers</a> /
                    <a href="#invited" class="hover:underline">Invited talk</a> /
                    <a href="#conference_i" class="hover:underline">International conference</a> /
                    <a href="#conference_j" class="hover:underline">Domestic conference</a> /
                    <a href="#book" class="hover:underline">Books</a> /
                    <a href="#kaisetsu" class="hover:underline">学会誌</a> /
                    <a href="#other" class="hover:underline">Others</a>
                </div>

                <div class="flex flex-wrap gap-x-4 gap-y-2 mb-8 text-blue-600">
                    <a href="https://scholar.google.co.jp/citations?user=qljXMcMAAAAJ&hl=ja" target="_blank" class="text-blue-600 hover:underline">Google Scholar</a> /
                    <a href="https://researchmap.jp/kojiinoue/?lang=japanese" target="_blank" class="text-blue-600 hover:underline">Research map</a>
                </div>

                <div>
                    <h2 id="journal" class="subsection-title">学術雑誌 (Journal papers)</h2>
                    <ul class="space-y-4 text-slate-800 publication-list">
                        <li>
                            Keiko Ochi, Divesh Lala, Koji Inoue, Tatsuya Kawahara, Hirokazu Kumazaki.<br>
                            Robot-Mediated Multi-Party Conversation Aimed at Affect Improvement for Psychiatric Patients.<br>
                            IEEE Transactions on Affective Computing, Vol. XX, No. XX, pp. XXXX-XXXX, 2025. <br>
                            [<a href="https://ieeexplore.ieee.org/document/11006954" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Kenta Yamamoto, Koji Inoue, Tatsuya Kawahara.<br>
                            Character expression of a conversational robot for adapting to user personality.<br>
                            Advanced Robotics, Vol. 38, No. 24, pp. 256-266, 2023. <br>
                            [<a href="https://www.tandfonline.com/doi/full/10.1080/01691864.2023.2285804" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://repository.kulib.kyoto-u.ac.jp/dspace/handle/2433/287441" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                        <li>
                            Yahui Fu, Koji Inoue, Divesh Lala, Kenta Yamamoto, Chenhui Chu, Tatsuya Kawahara.<br>
                            Dual variational generative model and auxiliary retrieval for empathetic response generation by conversational robot.<br>
                            Advanced Robotics, Vol. 37, No. 21, pp. 1406-1418, 2023. <br>
                            [<a href="https://www.tandfonline.com/doi/full/10.1080/01691864.2023.2270577" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://repository.kulib.kyoto-u.ac.jp/dspace/handle/2433/285997" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                        <li>
                            Keiko Ochi, Koji Inoue, Divesh Lala, Tatsuya Kawahara, Hirokazu Kumazaki.<br>
                            Effect of attentive listening robot on pleasure and arousal change in psychiatric daycare.<br>
                            Advanced Robotics, Vol. 37, No. 21, pp. 1382-1391, 2023. <br>
                            [<a href="https://www.tandfonline.com/doi/full/10.1080/01691864.2023.2257264" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://repository.kulib.kyoto-u.ac.jp/dspace/handle/2433/285730" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                         <li>
                            Kenta Yamamoto, Koji Inoue, Tatsuya Kawahara.<br>
                            Character expression for spoken dialogue systems with semi-supervised learning using Variational Auto-Encoder.<br>
                            Computer Speech & Language, Vol. 79, 101469, 2022. <br>
                            [<a href="https://www.sciencedirect.com/science/article/pii/S0885230822000924?via%3Dihub" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Koji Inoue, Divesh Lala, Tatsuya Kawahara.<br>
                            Can a robot laugh with you?: Shared laughter generation for empathetic spoken dialogue.<br>
                            Frontiers in Robotics and AI, 2022. <br>
                            [<a href="https://www.frontiersin.org/articles/10.3389/frobt.2022.933261/full" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://inokoj.github.io/slg/" target="_blank" class="text-blue-600 hover:underline">Demo page</a>]
                        </li>
                         <li>
                            井上昂治, ラーラー ディベッシュ, 山本賢太, 中村静, 高梨克也, 河原達也.<br>
                            アンドロイドERICAの傾聴対話システム--人間による傾聴との比較評価--.<br>
                            人工知能学会論文誌, Vol. 36, No. 5, pp. H-L51_1-12, 2021. <br>
                            [<a href="https://www.jstage.jst.go.jp/article/tjsai/36/5/36_36-5_H-L51/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Tatsuya Kawahara, Naoyuki Muramatsu, Kenta Yamamoto, Divesh Lala, Koji Inoue.<br>
                            Semi-autonomous avatar enabling unconstrained parallel conversations --seamless hybrid of WOZ and autonomous dialogue systems--.<br>
                            Advanced Robotics, Vol. 35, No. 11, pp. 657-663, 2021.<br>
                            [<a href="https://www.tandfonline.com/doi/full/10.1080/01691864.2021.1928549?scroll=top&needAccess=true" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>
                            井上昂治, 原康平, Divesh Lala, 山本賢太, 中村静, 高梨克也, 河原達也.<br>
                            掘り下げ質問を行う就職面接対話システムの自律型アンドロイドでの実装と評価.<br>
                            人工知能学会論文誌, Vol. 35, No. 5, pp. D-K43_1-10, 2020.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/tjsai/35/5/35_35-5_D-K43/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>
                            Yuanchao Li, Carlos T. Ishi, Koji Inoue, Shizuka Nakamura, Tatsuya Kawahara.<br>
                            Expressing reactive emotion based on multimodal emotion recognition for natural conversation in human-robot interaction.<br>
                            Advanced Robotics, Vol. 33, No 20, pp. 1030-1041, 2019.<br>
                            [<a href="https://www.tandfonline.com/doi/full/10.1080/01691864.2019.1667872" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Koji Inoue, Divesh Lala, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Engagement recognition by a latent character model based on multimodal listener behaviors in spoken dialogue.<br>
                            APSIPA Transaction on Signal and Information Processing, Vol. 7, No. e9, pp. 1-16, 2018.<br>
                            [<a href="https://www.cambridge.org/core/journals/apsipa-transactions-on-signal-and-information-processing/article/engagement-recognition-by-a-latent-character-model-based-on-multimodal-listener-behaviors-in-spoken-dialogue/E520CBA5B07362373D362F8CAD3E4696" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            山本賢太, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            人間型ロボットのキャラクタ表現のための対話の振る舞い制御モデル.<br>
                            人工知能学会論文誌, Vol. 33, No. 5, pp. C-l37_1-9, 2018. <br>
                            [<a href="https://doi.org/10.1527/tjsai.C-I37" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, Divesh Lala, 吉井和佳, 高梨克也, 河原達也.<br>
                            潜在キャラクタモデルによる聞き手のふるまいに基づく対話エンゲージメントの推定.<br>
                            人工知能学会論文誌, Vol. 33, No. 1, pp. DSH-F_1-12, 2018.<br>
                            [<a href="https://doi.org/10.1527/tjsai.DSH-F" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            山口貴史, 井上昂治, 吉野幸一郎, 高梨克也, Nigel G. Ward, 河原達也.<br>
                            傾聴対話システムのための言語情報と韻律情報に基づく多様な形態の相槌の生成.<br>
                            工知能学会論文誌, Vol. 31, No. 4, pp. C-G31_1-10, 2016.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/tjsai/31/4/31_C-G31/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Tatsuya Kawahara, Takuma Iwatate, Koji Inoue, Soichiro Hayashi, Hiromasa Yoshimoto, Katsuya Takanashi.<br>
                            Multi-modal sensing and analysis of poster conversations with smart posterboard.<br>
                            APSIPA Transaction on Signal and Information Processing, Vol. 5, No. e2, pp. 1-12, 2016.<br>
                            [<a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=10207240&fileId=S2048770316000020" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>
                            井上昂治, 若林佑幸, 吉本廣雅, 河原達也.<br>
                            多人数会話における音響・視線情報を統合した話者区間検出.<br>
                            電子情報通信学会論文誌, Vol. J99-D, No. 3, pp. 348-357, 2016.<br>
                            [<a href="http://search.ieice.org/bin/summary.php?id=j99-d_3_348&category=D&year=2016&lang=J&abst=" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            若林佑幸, 井上昂治, 中山雅人, 西浦敬信, 山下洋一, 吉本廣雅, 河原達也.<br>
                            視聴覚情報の統合に基づく音源数推定と話者ダイアライゼーション.<br>
                            電子情報通信学会論文誌, Vol. J99-D, No. 3, pp. 326-336, 2016.<br>
                            [<a href="http://search.ieice.org/bin/summary.php?id=j99-d_3_326&category=D&lang=J&year=2016&abst=" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Koji Inoue, Kohei Isechi, Hironobu Saito, Yoshimitsu Kuroki.<br>
                            An Inter-Prediction Method using Sparse Representation for High Efficiency Video Coding.<br>
                            IEICE Trans. on Fundamentals of Electronics, Communications and Computer Sciences, Vol. E96-A, No. 11, pp. 2191-2193, 2013.<br>
                            [<a href="http://search.ieice.org/bin/summary.php?id=e96-a_11_2191&amp;category=A&amp;year=2013&amp;lang=E&amp;abst=" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>
                            Kenjiro Sugimoto, Koji Inoue, Yoshimitsu Kuroki, Sei-ichiro Kamata.<br>
                            A linear manifold color descriptor for medicine package recognition.<br>
                            IEICE Trans. on Information and Systems, Vol. E95-D, No. 5, pp. 1264-1271, 2012.<br>
                            [<a href="http://search.ieice.org/bin/summary.php?id=e95-d_5_1264&amp;category=D&amp;year=2012&amp;lang=E&amp;abst=" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                    </ul>
                </div>

                <div>
                    <h2 id="invited" class="subsection-title">招待講演 (Invited talk)</h2>
                    <ul class="space-y-4 text-slate-800 publication-list">
                        <li>
                            Koji Inoue.<br>
                            Beyond Words: Non-Linguistic Behavior Generation for Human-like Conversational AI.<br>
                            International Workshop on Smart Info-Media Systems in Asia (SISA), 2024.<br>
                            [<a href="https://www.ieice-sisa.org/?page_id=6205" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Koji Inoue.<br>
                            Yeah, Well, Haha: Generating Non-linguistic Behaviors For Human-like Conversational Robots.<br>
                            SIGdial Meeting on Discourse and Dialogue (SIGDIAL), 2024.<br>
                            [<a href="https://2024.sigdial.org/keynotes/#koji" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://www.youtube.com/watch?v=2WYko6ZKWZg&t=1015s" target="_blank" class="text-blue-600 hover:underline">YouTube Live Archive</a>]
                        </li>
                        <li>
                            井上昂治.<br>
                            音声対話の魅力：相槌・笑い・ターンテイキング.<br>
                            日本音響学会 音声研究会, 電子情報通信学会 VNV研究会, 2024.
                        </li>
                        <li>
                            Koji Inoue.<br>
                            Closing the Gap: Exploring Human-Level Interaction in Android Robot Dialogue Systems.<br>
                            IEEE RO-MAN Workshop, Multidisciplinary Perspectives on COntext-aware embodied Spoken Interactions (MP-COSIN), 2023.<br>
                            [<a href="https://www.mpcosin.com/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治.<br>
                            アンドロイドERICAの音声対話システム～マルチモーダルチューリングテストへの挑戦～.<br>
                            音学シンポジウム, 2021.
                        </li>
                        <li>
                            井上昂治.<br>
                            空気が読める会話ロボット.<br>
                            情報処理学会全国大会 IPSJ-ONE 2019, March. 2019.<br>
                            [<a href="https://ipsj-one.org/2019/" target="_blank" class="text-blue-600 hover:underline">Link (講演動画あり)</a>]
                        </li>
                    </ul>
                </div>
                
                <div>
                     <h2 id="conference_i" class="subsection-title">国際会議 (International conference)</h2>
                     <ul class="space-y-4 text-slate-800 publication-list">
                         <li>
                             Mikey Elmers, Koji Inoue, Divesh Lala, Tatsuya Kawahara.<br>
                             Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems.<br>
                             INTERSPEECH, pp. xxxx-xxxx, 2025.<br>
                             Accepted
                         </li>
                         <li>
                             Noriki Nishida, Koji Inoue, Hideki Nakayama, Mayumi Bono, Katsuya Takanashi.<br>
                             Do Multimodal Large Language Models Truly See What We Point At? Investigating Indexical, Iconic, and Symbolic Gesture Comprehension.<br>
                             Annual Meeting of the Association for Computational Linguistics (ACL), pp. xxxx-xxxx, 2025.<br>
                             Accepted
                         </li>
                         <li>
                             Koji Inoue, Yuki Okafuji, Jun Baba, Yoshiki Ohira, Katsuya Hyodo, Tatsuya Kawahara.<br>
                             A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment.<br>
                             [<a href="https://arxiv.org/abs/2503.06241" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                         </li>
                         <li>
                             Koji Inoue, Mikey Elmers, Divesh Lala, Tatsuya Kawahara.<br>
                             Why Do We Laugh? Annotation and Taxonomy Generation for Laughable Contexts in Spontaneous Text Conversation.<br>
                             International Workshop on Spoken Dialogue Systems Technology (IWSDS), pp. 318-323, 2025.<br>
                             [<a href="https://aclanthology.org/2025.iwsds-1.34/" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://arxiv.org/abs/2501.16635" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>] [<a href="https://inokoj.github.io/slide/IWSDS2025_KI_Poster_Laugh.pdf" target="_blank" class="text-blue-600 hover:underline">Poster</a>]
                         </li>
                         <li>
                            Koji Inoue, Divesh Lala, Mikey Elmers, Keiko Ochi, Tatsuya Kawahara.<br>
                            An LLM Benchmark for Addressee Recognition in Multi-modal Multi-party Dialogue.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), pp. 330-334, 2025.<br>
                            [<a href="https://aclanthology.org/2025.iwsds-1.36/" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://arxiv.org/abs/2501.16643" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>] [<a href="https://inokoj.github.io/slide/IWSDS2025_KI_Poster_Multiparty.pdf" target="_blank" class="text-blue-600 hover:underline">Poster</a>]
                        </li>
                         <li>
                            Divesh Lala, Mikey Elmers, Koji Inoue, Zi Haur Pang, Keiko Ochi, Tatsuya Kawahara.<br>
                            ScriptBoard: Designing modern spoken dialogue systems through visual programming.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 176-182, 2025.<br>
                            [<a href="https://aclanthology.org/2025.iwsds-1.17/" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://github.com/DiveshLala/ScriptBoard" target="_blank" class="text-blue-600 hover:underline">GitHub</a>]
                        </li>
                        <li>
                            Koji Inoue, Divesh Lala, Gabriel Skantze, Tatsuya Kawahara.<br>
                            Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction with Fine-tuning of Voice Activity Projection.<br>
                            Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL), pp. 7171–7181, 2025.<br>
                            [<a href="https://aclanthology.org/2025.naacl-long.367/" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://arxiv.org/abs/2410.15929" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                        <li>
                            Zi Haur Pang, Yahui Fu, Divesh Lala, Mikey Elmers, Koji Inoue, Tatsuya Kawahara.<br>
                            Does the Appearance of Autonomous Conversational Robots Affect User Spoken Behaviors in Real-World Conference Interactions?.<br>
                            Late-Breaking Work at CHI Conference on Human Factors in Computing Systems (CHI EA), 2025.<br>
                            [<a href="https://dl.acm.org/doi/10.1145/3706599.3720179" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://arxiv.org/abs/2503.13625" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                        <li>
                            Zi Haur Pang, Yahui Fu, Divesh Lala, Mikey Elmers, Koji Inoue, Tatsuya Kawahara.<br>
                            Human-Like Embodied AI Interviewer: Employing Android ERICA in Real International Conference.<br>
                            International Conference on Computational Linguistics (COLING), 2025.<br>
                            [<a href="https://aclanthology.org/2025.coling-demos.14/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                            [<a href="https://arxiv.org/abs/2412.09867" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                        <li>
                            Divesh Lala, Koji Inoue, Tatsuya Kawahara.<br>
                            Prediction of Negative User Reactions Towards System Responses During Attentive Listening.<br>
                            APSIPA ASC, 2024.<br>
                            [<a href="http://www.apsipa2024.org/files/papers/483.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                         <li>
                            Divesh Lala, Koji Inoue, Haruki Kawai, Zi Haur Pang, Mikey Elmers, Tatsuya Kawahara.<br>
                            Development and Evaluation of a Semi-Autonomous Parallel Attentive Listening System.<br>
                            APSIPA ASC, 2024.<br>
                            [<a href="http://www.apsipa2024.org/files/papers/481.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>] 
                        </li>
                         <li>
                            Mikey Elmers, Koji Inoue, Divesh Lala, Keiko Ochi, Tatsuya Kawahara.<br>
                            Analysis and Detection of Differences in Spoken User Behaviors between Autonomous and Wizard-of-Oz Systems.<br>
                            Oriental COCOSDA (OCOCOSDA), 2024.<br>
                            [<a href="https://arxiv.org/abs/2410.03147" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                        <li>
                            Keiko Ochi, Koji Inoue, Divesh Lala, Tatsuya Kawahara.<br>
                            Entrainment Analysis and Prosody Prediction of Subsequent Interlocutor’s Backchannels in Dialogue.<br>
                            INTERSPEECH, pp. 462-466, 2024.<br>
                            [<a href="https://www.isca-archive.org/interspeech_2024/ochi24_interspeech.html" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Koji Inoue, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel Skantze.<br>
                            Multilingual Turn-taking Prediction Using Voice Activity Projection.<br>
                            Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING), pp. 11873-11883, 2024.<br>
                            [<a href="https://aclanthology.org/2024.lrec-main.1036/" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://arxiv.org/abs/2403.06487" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                        <li>
                            Koji Inoue, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel Skantze.<br>
                            Real-time and Continuous Turn-taking Prediction Using Voice Activity Projection.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2024.<br>
                            [<a href="https://arxiv.org/abs/2401.04868" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                        <li>
                            Koji Inoue, Divesh Lala, Keiko Ochi, Tatsuya Kawahara, Gabriel Skantze.<br>
                            An Analysis of User Behaviors for Objectively Evaluating Spoken Dialogue Systems.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2024.<br>
                            [<a href="https://arxiv.org/abs/2401.04867" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                        <li>
                            Zi Haur Pang, Yahui Fu, Divesh Lala, Keiko Ochi, Koji Inoue, Tatsuya Kawahara.<br>
                            Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2024.<br>
                            [<a href="https://arxiv.org/abs/2402.12770" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                        <li>
                            Haruki Kawai, Divesh Lala, Koji Inoue, Keiko Ochi, Tatsuya Kawahara.<br>
                            Evaluation of a semi-autonomous attentive listening system with takeover prompting.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2024.<br>
                            [<a href="https://arxiv.org/abs/2402.14863" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                         <li>
                            Sanae Yamashita, Koji Inoue, Ao Guo, Shota Mochizuki, Tatsuya Kawahara, Ryuichiro Higashinaka.<br>
                            RealPersonaChat: A Realistic Persona Chat Corpus with Interlocutors’ Own Personalities.<br>
                            Pacific Asia Conference on Language, Information and Computation (PACLIC), pp. 852–861, 2023.<br>
                            [<a href="https://aclanthology.org/2023.paclic-1.85/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Koji Inoue, Divesh Lala, Keiko Ochi, Tatsuya Kawahara, Gabriel Skantze.<br>
                            Towards Objective Evaluation of Socially-Situated Conversational Robots: Assessing Human-Likeness through Multimodal User Behaviors.<br>
                            International Conference on Multimodal Interaction (ICMI), Late-Breaking Results, Companion Proceedings, pp. 86–90, 2023.<br>
                            [<a href="https://dl.acm.org/doi/abs/10.1145/3610661.3617151" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://arxiv.org/abs/2308.11020" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                        <li>
                            Koji Inoue.<br>
                            Challenges and Approaches in Designing Social SDS in the LLM Era.<br>
                            Young Researchers Roundtable on Spoken Dialogue Systems (YRRSDS), 2023.<br>
                            [<a href="https://aclanthology.org/2023.yrrsds-1.8/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Yahui Fu, Koji Inoue, Chenhui Chu, Tatsuya Kawahara.<br>
                            Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation.<br>
                            SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pp. 645–656, 2023.<br>
                            [<a href="https://aclanthology.org/2023.sigdial-1.60/" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://arxiv.org/abs/2308.00085" target="_blank" class="text-blue-600 hover:underline">Preprint PDF</a>]
                        </li>
                        <li>
                            Sota Kobuki, Katie Seaborn, Seiki Tokunaga, Kosuke Fukumori, Shun Hidaka, Kazuhiro Tamura, Koji Inoue, Tatsuya Kawahara, Mihoko Otake-Matsuura. <br>
                            Robotic Backchanneling in Online Conversation Facilitation: A Cross-Generational Study.<br>
                            International Conference on Robot and Human Interactive Communication (RO-MAN), 2023.<br>
                            [<a href="https://ieeexplore.ieee.org/abstract/document/10309362" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Yuanchao Li, Koji Inoue, Leimin Tian, Changzeng Fu, Carlos Toshinori Ishi, Hiroshi Ishiguro, Tatsuya Kawahara, Catherine Lai.<br>
                            I Know Your Feelings Before You Do: Predicting Future Affective Reactions in Human-Computer Dialogue.<br>
                            CHI Conference on Human Factors in Computing Systems, Late-Breaking Work, 2023.<br> 
                            [<a href="https://dl.acm.org/doi/abs/10.1145/3544549.3585869" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Kenta Yamamoto, Koji Inoue, Tatsuya Kawahara.<br>
                            Character adaptation of spoken dialogue systems based on user personality.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2023.<br>
                            <strong class="text-red-500 font-bold">Best Paper Award</strong><br>
                            [<a href="https://drive.google.com/file/d/1Akt7Le2Hs_duK3JMctv-pDew3qEmmv1Y/view" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Yahui Fu, Koji Inoue, Divesh Lala, Kenta Yamamoto, Chenhui Chu, Tatsuya Kawahara.<br>
                            Improving Empathetic Response Generation with Retrieval based on Emotion Recognition.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2023.<br>
                            [<a href="https://drive.google.com/file/d/1SwG4YlSzmp8p6CaA1-dvlh9oH1KUYy18/view" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Yusuke Muraki, Divesh Lala, Haruki Kawai, Kenta Yamamoto, Koji Inoue, Tatsuya Kawahara.<br>
                            Semi-autonomous Guide Agents with Simultaneous Handling of Multiple Users.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2023.<br>
                            [<a href="https://drive.google.com/file/d/1secQzJdJaWkrCgAi6shoaKQhWxiFOF4N/view" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Divesh Lala, Koji Inoue, Kei Sawada, Tatsuya Kawahara.<br>
                            Backchannel Generation Model for a Third Party Listening Agent.<br>
                            International Conference on Human-Agent Interaction (HAI), pp. 114-122, 2022.<br>
                            [<a href="https://dl.acm.org/doi/abs/10.1145/3527188.3561926" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Seiki Tokunaga, Kohsuke Fukumori, Kazuhiro Tamura, Koji Inoue, Tatsuya Kawahara Mihoko Ohtake-Matsuura.<br>
                            Development of RobotHub: Integration of External System to Group Conversation System for Older Adults.<br>
                            World Conference of Gerontechnology (ISG), 2022.<br>
                            [<a href="https://journal.gerontechnology.org/archives/6d1a927c48db45d59f7c3ce765a1490a.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                         <li>
                            Haruki Kawai, Yusuke Muraki, Kenta Yamamoto, Divesh Lala, Koji Inoue, Tatsuya Kawahara.<br>
                            Simultaneous Job Interview System Using Multiple Semi-autonomous Agents.<br>
                            SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pp. 107-110, 2022.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/KAW-SIGDIAL22.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                         <li>
                            Seiya Kawano, Muteki Arioka, Akishige Yuguchi, Kenta Yamamoto, Koji Inoue, Tatsuya Kawahara, Satoshi Nakamura, Koichiro Yoshino.<br>
                            Multimodal Persuasive Dialogue Corpus using Teleoperated Android.<br>
                            INTERSPEECH, pp. 2918-2922, 2022.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/KAW-INTERSP22.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Yuanchao Li, Catherine Lai, Divesh Lala, Koji Inoue, Tatsuya Kawahara.<br>
                            Alzheimer’s Dementia Detection through Spontaneous Dialogue with Proactive Robotic Listeners.<br>
                            ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 875-879, 2022.<br>
                            [<a href="https://dl.acm.org/doi/abs/10.5555/3523760.3523896" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Koji Inoue, Hiromi Sakamoto, Kenta Yamamoto, Divesh Lala, Tatsuya Kawahara.<br>
                            A multi-party attentive listening robot which stimulates involvement from side participants.<br>
                            SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pp. 261-264, 2021.<br>
                            [<a href="https://sigdial.org/sites/default/files/workshops/conference22/Proceedings/pdf/2021.sigdial-1.28.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                         <li>
                            Divesh Lala, Koji Inoue, Kenta Yamamoto, Tatsuya Kawahara.<br>
                            Findings from human-android dialogue research with ERICA.<br>
                            RobotDial Workshop on Dialogue Models for Human-Robot Interaction (ROBOTDIAL), IJCAI-PRICAI workshop, 2021.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/ijcai2020/robotdial/09.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                         <li>
                            Divesh Lala, Koji Inoue, Tatsuya Kawahara.<br>
                            Prediction of shared laughter for human-robot dialogue.<br>
                            International Conference on Multimodal Interaction (ICMI), Companion Publication, pp. 62-66, 2020.<br>
                            [<a href="https://dl.acm.org/doi/10.1145/3395035.3425265" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LAL-ICMI20.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Koji Inoue, Kohei Hara, Divesh Lala, Kenta Yamamoto, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Job interviewer android with elaborate follow-up question generation.<br>
                            International Conference on Multimodal Interaction (ICMI), pp. 324-332, 2020.<br>
                            [<a href="https://dl.acm.org/doi/10.1145/3382507.3418839" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INO-ICMI20.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                         <li>
                            Kenta Yamamoto, Koji Inoue, Tatsuya Kawahara.<br>
                            Semi-supervised learning for character expression of spoken dialogue systems.<br>
                            INTERSPEECH, pp. 4188-4192, 2020.<br>
                            [<a href="https://www.isca-speech.org/archive/Interspeech_2020/abstracts/2293.html" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/YAM-INTERSP20.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Koji Inoue, Divesh Lala, Kenta Yamamoto, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            An attentive listening system with android ERICA: Comparison of autonomous and WOZ interactions.<br>
                            SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pp. 118-127, 2020.<br>
                            [<a href="https://www.aclweb.org/anthology/2020.sigdial-1.15/" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="https://www.sigdial.org/files/workshops/conference21/pdf/2020.sigdial-1.15.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Sota Isonishi, Koji Inoue, Divesh Lala, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Response generation to out-of-database questions for example-based dialogue systems.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2020.<br>
                            [<a href="https://link.springer.com/chapter/10.1007%2F978-981-15-8395-7_23" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/ISO-IWSDS20.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Kenta Yamamoto, Koji Inoue, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            A character expression model affecting spoken dialogue behaviors.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2020.<br>
                            [<a href="http://link-springer-com-443.webvpn.fjmu.edu.cn/chapter/10.1007%2F978-981-15-8395-7_1" target="_blank" class="text-blue-600 hover:underline">Link</a>] [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/YAM-IWSDS20.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Divesh Lala, Koji Inoue, Tatsuya Kawahara.<br>
                            Smooth turn-taking by a robot using an online continuous model to generate turn-taking cues.<br>
                            International Conference on Multimodal Interaction (ICMI), pp. 226-234, 2019.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LAL-ICMI19.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Kohei Hara, Koji Inoue, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Turn-taking Prediction Based on Detection of Transition Relevance Place.<br>
                            INTERSPEECH, pp. 4170-4179, 2019.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/HAR-INTERSP19.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Koji Inoue, Divesh Lala, Kenta Yamamoto, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Engagement-based adaptive behaviors for laboratory guide in human-robot dialogue.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2019.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INO-IWSDS19.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Koji Inoue, Kohei Hara, Divesh Lala, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            A job interview dialogue system with autonomous android ERICA.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2019.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/HAR-IWSDS19.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Koki Tanaka, Koji Inoue, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            End-to-end modeling for selection of utterance constructional units via system internal states.<br>
                            International Workshop on Spoken Dialogue Systems Technology (IWSDS), 2019.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/TAN-IWSDS19.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Kenta Yamamoto, Koji Inoue, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Dialogue Behavior Control Model for Expressing a Character of Humanoid Robots.<br>
                            APSIPA ASC, pp. 1732-1737, 2018.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/YAM-APSIPA18.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Divesh Lala, Koji Inoue, Tatsuya Kawahara.<br>
                            Evaluation of Real-time Deep Learning Turn-taking Models for Multiple Dialogue Scenarios.<br>
                            International Conference on Multimodal Interaction (ICMI), pp. 78-86, 2018.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LAL-ICMI18.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>] [<a href="https://dl.acm.org/citation.cfm?id=3242994" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Koji Inoue, Divesh Lala, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Engagement recognition in spoken dialogue via neural network by aggregating different annotators' models.<br>
                            INTERSPEECH, pp. 616-620, 2018.<br>
                            [<a href="https://www.isca-speech.org/archive/pdfs/interspeech_2018/inoue18b_interspeech.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Kohei Hara, Koji Inoue, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Prediction of turn-taking using multitask learning with prediction of backchannels and fillers.<br>
                            INTERSPEECH, pp. 991-995, 2018.<br>
                            [<a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1442.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Koji Inoue, Divesh Lala, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Latent Character Model for Engagement Recognition Based on Multimodal Behaviors.<br>
                            International Workshop on Spoken Dialogue Systems (IWSDS), 2018.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INO-IWSDS18.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Ryosuke Nakanishi, Koji Inoue, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Generating Fillers based on Dialog Act Pairs for Smooth Turn-Taking by Humanoid Robot.<br>
                            International Workshop on Spoken Dialogue Systems (IWSDS), May 2018.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/NAK-IWSDS18.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Tatsuya Kawahara, Koji Inoue, Divesh Lala, Katsuya Takanashi. <br>
                            Audio-visual conversation analysis by smart posterboard and humanoid robot. <br>
                            IEEE-ICASSP, pp. 6573-6577, April 2018.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/KAW-ICASSP18.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Hirofumi Inaguma, Masato Mimura, Koji Inoue, Kazuyoshi Yoshii, Tatsuya Kawahara.<br>
                            An end-to-end approach to joint social signal detection and automatic speech recognition.<br>
                            IEEE-ICASSP, pp. 6214-6218, April 2018.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INA-ICASSP18.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Yuanchao Li, Carlos Toshinori Ishi, Nigel Ward, Koji Inoue, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Emotion recognition by combining prosody and sentiment analysis for expressing reactive emotion by humanoid robot.<br>
                            APSIPA ASC, Dec. 2017.<br>
                             [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LYC-APSIPA17.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Divesh Lala, Koji Inoue, Pierrick Milhorat, Tatsuya Kawahara.<br>
                            Detection of social signals for recognizing engagement in human-robot interaction.<br>
                            AAAI Fall Symposium Natural Communication for Human-Robot Collaboration (NCHRC), Nov. 2017.<br>
                            [<a href="http://www.ttic.edu/nchrc/papers/6.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Hirofumi Inaguma, Koji Inoue, Masato Mimura, Tatsuya Kawahara.<br>
                            Social Signal Detection in Spontaneous Dialogue Using Bidirectional LSTM-CTC.<br>
                            INTERSPEECH, pp. 1691-1695, Aug. 2017.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INA-INTERSP17.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Divesh Lala, Milhorat Pierrick, Koji Inoue, Masanari Ishida, Tianyu Zhao, Tatsuya Kawahara.<br>
                            Attentive listening system with backchanneling, response generation and flexible turn-taking.<br>
                            SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pp. 127-136, Aug. 2017.<br>
                            [<a href="http://www.sigdial.org/workshops/conference18/proceedings/pdf/SIGDIAL16.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Pierrick Milhorat, Divesh Lala, Koji Inoue, Tianyu Zhao, Masanari Ishida, Katsuya Takanashi, Shizuka Nakamura, Tatsuya Kawahara.<br>
                            A conversational dialogue manager for the humanoid robot ERICA.<br>
                            International Workshop on Spoken Dialogue Systems (IWSDS), June 2017.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/MIL-IWSDS17.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Koji Inoue, Divesh Lala, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Annotation and analysis of listener's engagement based on multi-modal behaviors.<br>
                            ICMI workshop on Multimodal Analyses enabling Artificial Agents in Human-Machine Interaction (MA3HMI), Nov. 2016.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INO-ICMI-WS16.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>] [<a href="http://dl.acm.org/citation.cfm?id=3011271" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Hirofumi Inaguma, Koji Inoue, Shizuka Nakamura, Katsuya Takanashi, Tatsuya Kawahara.<br>
                            Prediction of ice-breaking between participants using prosodic features in the first meeting dialogue.<br>
                            ICMI workshop on Advancements in Social Signal Processing for Multimodal Interaction (ASSP4MI), Nov. 2016.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INA-ICMI-WS16.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>] [<a href="http://dl.acm.org/citation.cfm?id=3005472" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Divesh Lala, Pierrick Milhorat, Koji Inoue, Tianyu Zhao, Tatsuya Kawahara.<br>
                            Multimodal interaction with the autonomous android ERICA.<br>
                            ICMI, pp.417-418, Nov. 2016. (Demo)<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LAL-ICMI16.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>] [<a href="http://dl.acm.org/citation.cfm?id=2998528" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Koji Inoue, Pierrick Milhorat, Divesh Lala, Tianyu Zhao, Tatsuya Kawahara.<br>
                            Talking with ERICA, an autonomous android.<br>
                            SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pp.212-215, Sep. 2016. (Demo)<br>
                            [<a href="http://www.sigdial.org/workshops/conference17/proceedings/pdf/SIGDIAL25.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Tatsuya Kawahara, Takashi Yamaguchi, Koji Inoue, Katsuya Takanashi, Nigel G. Ward.<br>
                            Prediction and generation of backchannel form for attentive listening systems.<br>
                            INTERSPEECH, pp.2980-2894, Sep. 2016.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/KAW-INTERSP16.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Takashi Yamaguchi, Koji Inoue, Koichiro YoshiNo. Katsuya Takanashi, Nigel G. Ward, Tatsuya Kawahara.<br>
                            Analysis and prediction of morphological patterns of backchannels for attentive listening agents.<br>
                            International Workshop on Spoken Dialog Systems (IWSDS), Jan. 2016.<br>
                            [<a href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/YAM-IWSDS16.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Koji Inoue, Yukoh Wakabayashi, Hiromasa Yoshimoto, Katsuya Takanashi, and Tatsuya Kawahara.<br>
                            Enhanced speaker diarization with detection of backchannels using eye-gaze information in poster conversations.<br>
                            INTERSPEECH, pp.3086-3090, Sep. 2015. (Oral)<br>
                            [<a href="http://www.sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INO-INTERSP15.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Yukoh Wakabayashi, Koji Inoue, Hiromasa Yoshimoto, and Tatsuya Kawahara.<br>
                            Speaker Diarization based on Audio-Visual Integration for Smart Posterboard.<br>
                            APSIPA ASC, FP2-1-1116, Dec. 2014. (Oral)
                        </li>
                        <li>
                            Koji Inoue, Yukoh Wakabayashi, Hiromasa Yoshimoto, and Tatsuya Kawahara.<br>
                            Speaker Diarization using Eye-gaze Information in Multi-party Conversations.<br>
                            INTERSPEECH, pp.562-566, Sep. 2014. (Poster)<br>
                            [<a href="http://www.sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/INO-INTERSP14.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            Randy Gomez, Koji Inoue, Keisuke Nakamura, Takeshi Mizumoto, and Kazuhiro Nakadai.<br>
                            Speech-based Human-Robot Interaction Robust to Acoustic Reflections in Real Environment.<br>
                            International Conference on Intelligent Robots and Systems (IROS), pp. 1367-1373, 2014.
                        </li>
                        <li>
                            Koji Inoue, Hironobu Saito, and Yoshimitsu Kuroki.<br>
                            Local intensity compensation using sparse representation.<br>
                            International Conference on Pattern Recognition (ICPR), No. TuAT2.4, pp. 951-954, Tsukuba, Japan, Nov. 2012. (Oral, Oral acceptance rate = 16.1%, 313/1941)<br>
                            [<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6460292" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Koji Inoue and Yoshimitsu Kuroki.<br>
                            On a constraint condition of sparse representation for illumination-robust face recognition.<br>
                            International Workshop on Image & Signal Processing and Retrieval (IWISPR 2012), Oct. 2012. (Poster)
                        </li>
                        <li>
                            Kohei Isechi, Koji Inoue, and Yoshimitsu Kuroki.<br>
                            Video compression using sparse representation.<br>
                            International Workshop on Image & Signal Processing and Retrieval (IWISPR), Oct. 2012. (Poster)
                        </li>
                        <li>
                            Koji Inoue, Hironobu Saito, and Yoshimitsu Kuroki.<br>
                            Motion compensation using sparse representation.<br>
                            2012 International Workshop on Smart Info-Media Systems in Asia (SISA 2012), RS3-10, Sep. 2012. (Poster)
                        </li>
                        <li>
                            Koji Inoue and Yoshimitsu Kuroki.<br>
                            On sparse representation for face recognition under illumination change.<br>
                            International Workshop on Advanced Image Technology (IWAIT), pp. 662-667, Jan. 2012. (Poster)
                        </li>
                        <li>
                            Shoma Eguchi, Koji Inoue, Yoshimitsu Kuroki, Masayuki Kurosaki, Yuhei Nagao, and Hiroshi Ochi.<br>
                            On Parallel 2D-DWT of JPEG 2000 conformed to Digital Cinema Initiatives using GPGPU.<br>
                            International Workshop on Advanced Image Technology (IWAIT), pp. 668-671, Jan. 2012. (Poster)
                        </li>
                        <li>
                            Koji Inoue and Yoshimitsu Kuroki.<br>
                            Illumination-Robust Face Recognition via Sparse Representation.<br>
                            Visual Communication and Image Processing (VCIP), O-10.2, Nov. 2011. (Oral)<br>
                            [<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=06115969" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Koji Inoue and Yoshimitsu Kuroki.<br>
                            Face Recognition under Illumination Change using Sparse Representation.<br>
                            International Workshop on Target Recognition and Tracking (IWTRT), Oct. 2011. (Poster)
                        </li>
                        <li>
                            Shoma Eguchi, Koji Inoue, and Yoshimitsu Kuroki.<br>
                            Real Time 2D-DWT of JPEG 2000 for Digital Cinema using Multiple GPUs.<br>
                            International Workshop on Target Recognition and Tracking (IWTRT), Oct. 2011. (Poster)
                        </li>
                        <li>
                            Koji Inoue, Yoshimitsu Kuroki, Masayuki Kurosaki, Yuhei Nagao, Baiko Sai, and Hiroshi Ochi.<br>
                            A parallel Computing using CUDA for the 2D-DWT of JPEG 2000.<br>
                            International Symposium on Multimedia and Communication Technology (ISMAC), TS1-5, Sep. 2011. (Oral)
                        </li>
                        <li>
                            Kenjiro Sugimoto, Koji Inoue, Yoshimitsu Kuroki, and Sei-ichiro Kamata.<br>
                            A color distribution descriptor for medicine package recognition.<br>
                            China-Japan-Korea Joint Workshop on Pattern Recognition (CJKPR), pp. 64-69, Nov. 2010. (Poster)
                        </li>
                    </ul>
                </div>
                <!-- 他のセクションも同様に変換 -->
                
                <div>
                    <h2 id="conference_j" class="subsection-title">全国大会・研究会</h2>
                     <ul class="space-y-4 text-slate-800 publication-list">
                        <li>
                            坂井田瑠衣, 井上昂治, 坂尾結衣, 中林由希子, 横森大輔, 高梨克也.<br>
                            マルチモーダルコーパスの移動場面に対する動作連鎖アノテーションの試み：未来館SCコーパスを対象に.<br>
                            人工知能学会研究会資料, SLUD-103-35, 2025.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/103/0/103_195/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, Divesh Lala, Mikey Elmers, 越智景子, 河原達也<br>
                            多人数対話における受話者推定のLLMによる試み.<br>
                            人工知能学会研究会資料, SLUD-103-34, 2025.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/103/0/103_189/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            福重茜, 井上昂治, 河原達也, 山下紗苗, 東中竜一郎.<br>
                            多人数チャットコーパスにおける参加者間の関係性の推定.<br>
                            人工知能学会研究会資料, SLUD-103-33, 2025.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/103/0/103_183/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            竹内一央, 井上昂治, 河原 達也.<br>
                            LLMによる対話における驚き反応の生成.<br>
                            人工知能学会研究会資料, SLUD-103-32, 2025.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/103/0/103_177/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            加藤利梓, 井上昂治, 河原 達也.<br>
                            アバター傾聴対話システムにおける多様な頷きのリアルタイム予測.<br>
                            人工知能学会研究会資料, SLUD-103-31, 2025.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/103/0/103_171/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            津田太郎, 山下紗苗, 井上昂治, 河原達也, 東中竜一郎.<br>
                            Multi-Relational Multi-Party Chat Corpus: 話者間の関係性に着目したマルチパーティ雑談対話コーパス.<br>
                            言語処理学会第31回年次大会, D10-6, 2025.<br>
                            [<a href="https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/D10-6.pdf" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, Mikey Elmers, Divesh Lala, 河原達也.<br>
                            人はなぜ笑うのか？対話における笑いの根拠ラベルの半自動構築.<br>
                            言語処理学会第31回年次大会, D9-6, 2025.<br>
                            [<a href="https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/D9-6.pdf" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>
                            西田典起, 井上昂治,中山英樹, 坊農真弓, 高梨克也.<br>
                            マルチモーダル大規模言語モデルはジェスチャーをどこまで理解しているのか：指標性・図像性・象徴性を問う.<br>
                            言語処理学会第31回年次大会, C3-5, 2025.<br>
                            [<a href="https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/C3-5.pdf" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, Lala Divesh, Skantze Gabriel, 河原 達也.<br>
                            Voice Activity Projectionモデルを用いたリアルタイム相槌予測.<br>
                            人工知能学会研究会資料, SLUD-102-229, 2024.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/102/0/102_229/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            住田 龍宇一, 井上昂治, 河原達也. <br>
                            RAGチャットボットは重要でない会話を忘れるべきか？ 心理学における発見を用いた重要度と忘却の探究.<br>
                            人工知能学会研究会資料, SLUD-102-28, 2024.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/102/0/102_28/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]<br>
                            <strong class="text-red-500 font-bold">第15回対話システムシンポジウム若手優秀賞</strong>
                        </li>
                        <li>
                            彭 子豪, 傅 雅慧, ラーラー ディベッシュ, エルマーズ マイキー, 井上 昂治, 河原 達也<br>
                            アンドロイドERICAによる国際会議におけるインタビュー対話システム.<br>
                            人工知能学会研究会資料, SLUD-102-01, 2024.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/102/0/102_01/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>
                            彭子豪, 傅雅慧, Divesh Lala, 越智景子, 井上昂治, 河原達也. <br>
                            感情語りコーパスを用いたバリデーション応答の生成.<br>
                            人工知能学会研究会資料, SLUD-099-23, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/99/0/99_119/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>
                            越智景子, 井上昂治, Divesh Lala, 河原達也, 熊崎博一. <br>
                            精神科デイケアのための傾聴対話システム：きくロボ.<br>
                            人工知能学会研究会資料, SLUD-099-16, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/99/0/99_78/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            川井悠生, 山本賢太, 越智景子, Lala Divesh, 井上昂治, 河原達也. <br>
                            半自律対話による同時並列傾聴システム.<br>
                            人工知能学会研究会資料, SLUD-099-14, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/99/0/99_66/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]<br>
                            <strong class="text-red-500 font-bold">第14回対話システムシンポジウム若手優秀賞</strong>
                        </li>
                        <li>
                            井上昂治, Divesh Lala, 越智景子, 河原達也, Gabriel Skantze. <br>
                            音声対話システムの客観評価のためのユーザのマルチモーダルなふるまいの分析.<br>
                            人工知能学会研究会資料, SLUD-099-01, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/99/0/99_01/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Prediction of Validating Response from Emotional Storytelling Corpus.<br>
                            Zi Haur Pang, Yahui Fu, Divesh Lala, Keiko Ochi, Koji Inoue, Tatsuya Kawahara.<br>
                            第37回 人工知能学会全国大会, 2023. <br>
                            [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2023/0/JSAI2023_2O5OS2a03/_pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            越智景子, 井上昂治, 河原達也, 大西祐美, 熊﨑博一.<br>
                            傾聴ロボットを用いた精神科デイケアでの会話の基礎検討.<br>
                            第41回 日本社会精神医学会, 2023. 
                        </li>
                        <li>
                            三村正人, 井上昂治, 河原達也, 中村友彦, 猿渡洋.<br>
                            実環境下日本語話し言葉音声コーパスの構築と音声認識ベンチマーク.<br>
                            情報処理学会研究報告, SLP-146-12, 2023.<br>
                            [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=224409&item_no=1&page_id=13&block_id=8" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>
                            山本賢太, 井上昂治, 河原達也. <br>
                            音声対話システムによる雑談対話におけるキャラクタ表現に基づくユーザ適応の検討.<br>
                            人工知能学会研究会資料, SLUD-096-29, 2022.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/96/0/96_29/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            越智景子, 井上昂治, Lala Divesh, 河原達也.<br>
                            傾聴対話におけるユーザ発話に同調する相槌の韻律制御.<br>
                            日本音響学会2022秋季研究発表会, 3-8-9, 2022. 
                        </li>
                        <li>
                            有岡無敵, 山本賢太, 井上昂治, 河原達也, 中村哲, 吉野幸一郎.<br>
                            遠隔操作アンドロイドを用いたマルチモーダル説得対話コーパスの収集と分析.<br>
                            言語処理学会第28回年次大会, B2-2, 2022.
                        </li>
                         <li>
                            川井悠生, 山本賢太, Lala Divesh, 井上昂治, 河原達也.<br>
                            回答評価とキーワード抽出を用いた同時並列就職面接対話システム.<br>
                            情報処理学会 第84回全国大会, 2R-05, 2022.<br>
                            <strong class="text-red-500 font-bold">学生奨励賞</strong>
                        </li>
                        <li>
                            長連成, 井上昂治, 越智景子, 河原達也.<br>
                            大規模テキストデータを用いた事前学習による音声対話の相槌予測.<br>
                            情報処理学会 第84回全国大会, 2R-04, 2022.
                        </li>
                        <li>
                            村木優介, 山本賢太, Lala Divesh, 井上昂治, 河原達也.<br>
                            半自律並列プレゼン対話システムのための質問分類による介入タイミング制御.<br>
                            情報処理学会 第84回全国大会, 2R-03, 2022.
                        </li>
                        <li>
                            井上昂治, Lala Divesh, 河原達也. <br>
                            共感を表出する音声対話システムのための共有笑い生成.<br>
                            人工知能学会研究会資料, SLUD-093-27, 2021.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/93/0/93_155/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            山本賢太, 井上昂治, 河原達也. <br>
                            音声対話システムのユーザ適応に向けたパーソナリティの関係性の分析.<br>
                            人工知能学会研究会資料, SLUD-093-02, 2021.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jsaislud/93/0/93_09/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            酒井和紀, 吉川雄一郎, 井上昂治, 河原達也, 石黒浩. <br>
                            複数ロボットによる対話継続効果検証のための商業施設でのフィールド実験.<br>
                            第39回日本ロボット学会学術講演会, 2B4-02, 2021．
                        </li>
                        <li>
                            井上昂治.<br>
                            アンドロイドERICAの音声対話システム～マルチモーダルチューリングテストへの挑戦～.<br>
                            電子情報通信学会技術研究報告, SP2021-7, 2021. 
                        </li>
                        <li>
                            坂本寛弥, 山本賢太, Lala Divesh, 井上昂治, 河原達也.<br>
                            話し手以外のユーザへの参与促進発話を生成する多人数傾聴対話システム.<br>
                            情報処理学会全国大会講演論文集, 6N-06, 2021.
                        </li>
                        <li>
                            井上昂治, Lala Divesh, 河原達也.<br>
                            ヒューマンロボットインタラクションのための相槌・笑いのリアルタイム検出.<br>
                            日本音響学会2021春季研究発表会, 1-2-9, 2021. 
                        </li>
                        <li>
                            井上昂治, Lala Divesh, 河原達也.<br>
                            ヒューマンロボットインタラクションにおける音響特徴に基づく共有笑いの予測.<br>
                            日本音響学会2021春季研究発表会, 1-2-8, 2021. 
                        </li>
                        <li>
                            山本賢太, 井上昂治, 河原達也. <br>
                            VAEを用いた半教師あり学習による音声対話システムのためのキャラクタ表現.<br>
                            人工知能学会研究会資料, SLUD-C002-31, 2020.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=10940" target="_blank" class="text-blue-600 hover:underline">Link</a>]<br>
                            <strong class="text-red-500 font-bold">第11回対話システムシンポジウム若手優秀賞</strong>
                        </li>
                        <li>
                            田中滉己, 井上昂治, 中村静, 高梨克也, 河原達也. <br>
                            多様なふるまいからの好感判定に基づき発話構成要素を選択するお見合い対話システム.<br>
                            人工知能学会研究会資料, SLUD-C002-28, 2020.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=10937" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, Lala Divesh, 山本賢太, 中村静, 高梨克也, 河原達也. <br>
                            アンドロイドERICAの傾聴対話システムにおけるWOZとの比較評価.<br>
                            人工知能学会研究会資料, SLUD-C002-21, 2020.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=10929" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, Lala Divesh, 山本賢太, 中村静, 高梨克也, 河原達也.<br>
                            WOZとの比較による自律型アンドロイドERICAの傾聴対話システムの評価.<br>
                            日本音響学会2020秋季研究発表会, 3-2-7, 2020. 
                        </li>
                        <li>
                            井上昂治, 原康平, Lala Divesh, 山本賢太, 中村静, 高梨克也, 河原達也.<br>
                            自律型アンドロイドERICAによる就職面接対話.<br>
                            日本音響学会2020春季研究発表会, 3-4-3, 2020. 
                        </li>
                        <li>
                            磯西爽太, 井上昂治, 高梨克也, 河原達也. <br>
                            質問タイプの分類に基づく登録外質問に対する応答生成.<br>
                            人工知能学会研究会資料, SLUD-B902-06, 2019.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=10546" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, Divesh Lala, 山本賢太, 中村静, 高梨克也, 河原達也. <br>
                            自律型アンドロイドERICAによる傾聴対話システムの評価.<br>
                            人工知能学会研究会資料, SLUD-B902-04, 2019.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=10544" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>
                            原康平, 井上昂治, Divesh Lala, 山本賢太, 中村静, 高梨克也, 河原達也. <br>
                            アンドロイドERICAによる面接対話における掘り下げ質問生成.<br>
                            人工知能学会研究会資料, SLUD-B902-03, 2019.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=10543" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            春日悠生, 井上昂治, 山本賢太, 高梨克也, 河原達也. <br>
                            ヒューマンロボットインタラクションコーパスへの焦点アノテーションの基準と予備的分析.<br>
                            人工知能学会研究会資料, SLUD-B901-03, 2019.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&page_id=13&active_action=repository_view_main_item_detail&item_id=10353&item_no=1&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]<br>
                            <strong class="text-red-500 font-bold">人工知能学会 2019年度研究会優秀賞</strong>
                        </li>
                        <li>
                            井上昂治, Lala Divesh, 山本賢太, 中村静, 高梨克也, 河原達也.<br>
                            自律型アンドロイドERICAによる傾聴対話の評価.<br>
                            日本音響学会2019秋季研究発表会, 1-3-2, 2019. <br>
                            <strong class="text-red-500 font-bold">日本音響学会粟屋潔学術奨励賞</strong>
                        </li>
                        <li>
                            山本賢太, 井上昂治, 中村静, 高梨克也, 河原達也. <br>
                            対話のふるまい制御によるキャラクタ表現モデルと対話コーパスによる検証.<br>
                            情報処理学会全国大会講演論文集, 2T-07, 2019.
                        </li>
                        <li>
                            田中滉己, 井上昂治, 中村静, 高梨克也, 河原達也. <br>
                            対話相手への好感に基づく発話構成要素の選択とお見合い対話システムへの実装.<br>
                            情報処理学会全国大会講演論文集, 2T-06, 2019.<br>
                            <strong class="text-red-500 font-bold">大会優秀賞</strong>, <strong class="text-red-500 font-bold">学生奨励賞</strong>
                        </li>
                        <li>
                            磯西爽太, 田中滉己, 井上昂治, 高梨克也, 河原達也. <br>
                            質問タイプの分類に基づく用例なし質問に対する応答生成.<br>
                            情報処理学会全国大会講演論文集, 2T-05, 2019.
                        </li>
                        <li>
                            原康平, 井上昂治, 高梨克也, 河原達也. <br>
                            移行適格場の予測に基づくターンテイキング予測.<br>
                            情報処理学会全国大会講演論文集, 2T-04, 2019.
                        </li>
                        <li>
                            原康平, 井上昂治, 高梨克也, 河原達也. <br>
                            移行適格場の情報を考慮したターンテイキング予測.<br>
                            人工知能学会研究会資料, SLUD-B803-10, 2019.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9710&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>
                            山本賢太, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            対話のふるまいに基づくキャラクタ表現の対話コーパスにおける分析.<br>
                            人工知能学会研究会資料, SLUD-B803-08, 2019.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9708&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]<br>
                            <strong class="text-red-500 font-bold">人工知能学会 2018年度研究会優秀賞</strong>
                        </li>
                        <li>
                            井上昂治, Divesh Lala, 原康平, 相原邦光, 中村静, 高梨克也, 河原達也. <br>
                            自律型アンドロイドERICAによる就職面接対話.<br>
                            人工知能学会研究会資料, SLUD-B802-14, 2018.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9621&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            山本賢太, 井上昂治, Divesh Lala, 中村静, 高梨克也, 河原達也. <br>
                            自律型アンドロイドERICAによる傾聴対話.<br>
                            人工知能学会研究会資料, SLUD-B802-13, 2018.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9620&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>
                            田中滉己, 井上昂治, 中村静, 高梨克也, 河原達也. <br>
                            初対面対話における好感のモデリングと発話構成要素の選択.<br>
                            人工知能学会研究会資料, SLUD-B802-03, 2018.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9609&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            原康平, 井上昂治, 高梨克也, 河原達也. <br>
                            相槌・フィラー予測とのマルチタスク学習によるターンテイキング予測.<br>
                            人工知能学会研究会資料, SLUD-B802-01, 2018.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9607&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>
                            原康平, 井上昂治, 高梨克也, 河原達也.<br>
                            相槌・フィラー予測とのマルチタスク学習による円滑なターンテイキング.<br>
                            第80回情報処理学会全国大会, 6Q-07, 2018. 
                        </li>
                        <li>
                            山本賢太, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            自律型アンドロイドのキャラクタ表現のための対話の振る舞い制御モデルの構築と評価.<br>
                            第80回情報処理学会全国大会, 6Q-06, 2018. <br>
                            <strong class="text-red-500 font-bold">学生奨励賞</strong>
                        </li>
                         <li>
                            田中滉己, 井上昂治, 高梨克也, 河原達也.<br>
                            初対面対話における好感の生成と発話構成要素の予測のモデル.<br>
                            第80回情報処理学会全国大会, 6Q-05, 2018. <br>
                            <strong class="text-red-500 font-bold">学生奨励賞</strong>
                        </li>
                        <li>
                            石田真也, 井上昂治, 中村静, 高梨克也, 河原達也. <br>
                            共感・発話促進のための多様な聞き手応答を生成する傾聴対話システム.<br>
                            第80回情報処理学会全国大会, 6Q-04, 2018. 
                        </li>
                         <li>
                            井上昂治, Lala Divesh, 高梨克也, 河原達也.<br>
                            自律型アンドロイドERICAにおけるエンゲージメント推定に基づく音声対話システム.<br>
                            日本音響学会2018春季研究発表会, 2-8-9, 2018. 
                        </li>
                        <li>
                            石田真也, 井上昂治, 中村静, 高梨克也, 河原達也. <br>
                            共感表出と発話促進のための聞き手応答を生成する傾聴対話システム.<br>
                            人工知能学会研究会資料, SLUD-B509-2, 2018.<br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=9072&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, Lala Divesh, Milhorat Pierrick, 高梨克也, 河原達也.<br>
                            潜在キャラクタモデルによるリアルタイム対話エンゲージメント推定.<br>
                            人工知能学会研究会資料, SLUD-B508-18, 2017. <br>
                            [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=8937&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]<br>
                            <strong class="text-red-500 font-bold">若手奨励賞</strong>, <strong class="text-red-500 font-bold">人工知能学会 2017年度研究会優秀賞</strong>
                        </li>
                        <li>
                            井上昂治, Lala Divesh, Milhorat Pierrick, 石田真也, 趙天雨, 高梨克也, 河原達也.<br>
                            自律型アンドロイドERICAにおける多様な聞き手応答を用いた傾聴対話.<br>
                             人工知能学会研究会資料, SLUD-B508-11, 2017.<br>
                             [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=8928&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            山本賢太, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            自律型アンドロイドの対話の振る舞い制御モデルによる キャラクタ表現法の検討.<br>
                             人工知能学会研究会資料, SLUD-B508-05, 2017.<br>
                              [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=8921&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, Lala Divesh, 吉井和佳, 高梨克也, 河原達也.<br>
                            潜在キャラクタモデルによる聞き手のふるまいに基づく対話エンゲージメントの推定.<br>
                             日本音響学会2017秋季研究発表会, 2-Q-12, 2017.
                        </li>
                        <li>
                            稲熊寛文, 井上昂治, 三村正人, 河原達也.<br>
                            End-to-endモデルによる音声対話中のsocial signalsの検出.<br>
                             日本音響学会2017秋季研究発表会, 1-10-16, 2017. <strong class="text-red-500 font-bold">学生優秀発表賞</strong>
                        </li>
                        <li>
                            稲熊寛文, 井上昂治, 三村正人, 河原達也.<br>
                            End-to-endモデルによるsocial signals検出および音声認識との統合.<br>
                            情報処理学会研究報告, SLP-117-7, 2017.
                        </li>
                        <li>
                            勝見久央, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            自律型アンドロイドによる対話における同調的笑いの生成.<br>
                            情報処理学会研究報告, SLP-116-4, 2017. <strong class="text-red-500 font-bold">学生奨励賞</strong>
                        </li>
                        <li>
                            稲熊寛文, 井上昂治, 河原達也.<br>
                            ニューラルネットによる音声対話における非言語的振る舞いの検出.<br>
                            第79回情報処理学会全国大会, 7M-04, 2017. <strong class="text-red-500 font-bold">学生奨励賞</strong>
                        </li>
                        <li>
                            勝見久央, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            自律型アンドロイドによる対話における「同調的笑い」の生成.<br>
                            第79回情報処理学会全国大会, 7M-03, 2017.
                        </li>
                        <li>
                            石田真也, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            傾聴対話システムにおける自分語りを含む多様な聞き手応答の生成.<br>
                             第79回情報処理学会全国大会, 7M-02, 2017. 
                        </li>
                        <li>
                            山本賢太, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            自律型アンドロイドのキャラクタ表現のための対話の振る舞い制御.<br>
                            第79回情報処理学会全国大会, 7M-01, 2017.
                        </li>
                        <li>
                            井上昂治, Lala Divesh, 高梨克也, 河原達也.<br>
                            聞き手の多様なふるまいに基づく対話エンゲージメントの推定.<br>
                            日本音響学会2017春季研究発表会, 3-5-1, 2017.
                        </li>
                        <li>
                            井上昂治, 三村正人, 石井カルロス寿憲, 坂井信輔, 河原達也.<br>
                            DAEを用いたリアルタイム遠隔音声認識.<br>
                             日本音響学会2017春季研究発表会, 1-Q-6, 2017.
                        </li>
                        <li>
                            李遠超, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            ヒューマンロボットインタラクションにおける韻律とテキスト情報を組み合わせた感情認識と評価応答選択.<br>
                             人工知能学会研究会資料, SLUD-B505-09, 2017. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=8547&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            中西亮輔, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            円滑な発話権制御のための談話行為の連鎖に基づく フィラーの生起と形態の予測.<br>
                            人工知能学会研究会資料, SLUD-B506-04, 2017. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=8542&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            稲熊寛文, 井上昂治, 三村正人, 河原達也.<br>
                            LSTM-CTCによる音声対話におけるSocial Signalsの検出.<br>
                            情報処理学会研究報告, SLP-115-9, 2017. [<a href="https://webcache.googleusercontent.com/search?q=cache:f9M9SWZsyx8J:https://ipsj.ixsq.nii.ac.jp/ej/%3Faction%3Drepository_uri%26item_id%3D177382+&cd=1&hl=ja&ct=clnk&gl=jp" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            中西亮輔, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            発話行為の連鎖を考慮したフィラーの生起と形態の分析.<br>
                            人工知能学会研究会資料, SLUD-B505-30, 2016. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=1310&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            稲熊寛文, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            初対面対話における韻律的特徴に基づくアイスブレーキングの分析と予測.<br>
                             人工知能学会研究会資料, SLUD-B505-29, 2016. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=1309&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, Lala Divesh, 高梨克也, 河原達也.<br>
                            階層ベイズモデルを用いた聞き手の多様なふるまいに基づく対話エンゲージメントの推定.<br>
                             人工知能学会研究会資料, SLUD-B505-28, 2016. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=1308&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, Milohorat Pierrick, Lala Divesh, 趙天雨, 河原達也.<br>
                            自律型アンドロイドERICAによる社会的役割に則したインタラクション.<br>
                             人工知能学会研究会資料, SLUD-B505-7, 2016. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=1288&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            石田真也, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            傾聴対話システムのための発話を促す聞き手応答の生成.<br>
                            人工知能学会研究会資料, SLUD-B504-01, 2016. [<a href="https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=1147&item_no=1&page_id=13&block_id=23" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            稲熊寛文, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            初対面対話における場の和みのマルチモーダルな分析と検出.<br>
                            第78回情報処理学会全国大会, 6Q-02, 2016. 
                        </li>
                        <li>
                            石田真也, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            傾聴対話システムのための多様な聞き手応答の生成.<br>
                             第78回情報処理学会全国大会, 6Q-01, 2016. <strong class="text-red-500 font-bold">学生奨励賞</strong>
                        </li>
                        <li>
                            井上昂治, 三村正人, 石井カルロス寿憲, 河原達也.<br>
                            自律型アンドロイドERICAのための遠隔音声認識.<br>
                            日本音響学会2016春季研究発表会, 1-1-1, 2016. <strong class="text-red-500 font-bold">学生優秀発表賞</strong>
                        </li>
                        <li>
                            中西亮輔, 井上昂治, 中村静, 高梨克也, 河原達也.<br>
                            自律型アンドロイドによる円滑な発話権制御のためのフィラーの生起位置と形態の分析.<br>
                            人工知能学会研究会資料, SLUD-B503-11, 2016.
                        </li>
                        <li>
                            山口貴史, 井上昂治, 吉野幸一郎, 高梨克也, Ward G. Nigel, 河原達也.<br>
                            傾聴対話システムのための言語情報と韻律情報に基づく多様な形態の相槌の生成.<br>
                             人工知能学会研究会資料, SLUD-B503-9, 2016.
                        </li>
                        <li>
                            井上昂治, 河原達也.<br>
                            自律型アンドロイドEricaのための音声対話システム.<br>
                            人工知能学会研究会資料, SLUD-B502-5, 2015. [<a href="http://sap.ist.i.kyoto-u.ac.jp/erato/INO-slud15-10.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            山口貴史, 井上昂治, 吉野幸一郎, 高梨克也, Ward G. Nigel, 河原達也.<br>
                            多様な相槌をうつ傾聴対話システムのための相槌形態の予測.<br>
                            人工知能学会研究会資料, SLUD-B502-1, 2015.
                        </li>
                        <li>
                            井上昂治, 若林佑幸, 吉本廣雅, 高梨克也, 河原達也.<br>
                            ポスター会話における音響・視線情報の確率的統合による 話者区間及び相槌の検出.<br>
                             日本音響学会2015秋季研究発表会, 2-2-4, 2015.
                        </li>
                        <li>
                            井上昂治, 若林佑幸, 吉本廣雅, 高梨克也, 河原達也.<br>
                            スマートポスターボードにおける視線情報を用いた話者区間及び相槌の検出.<br>
                             情報処理学会研究報告, MUS-107-68, 2015. [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=142054&item_no=1&page_id=13&block_id=8" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, 若林佑幸, 吉本廣雅, 高梨克也, 河原達也.<br>
                            スマートポスターボードにおける視線情報を用いた話者区間検出及び相槌の同定.<br>
                            第77回情報処理学会全国大会, 6P-09, 2015. <strong class="text-red-500 font-bold">学生奨励賞</strong>
                        </li>
                        <li>
                            若林佑幸, 中山雅人, 西浦敬信, 山下洋一, 井上昂治, 吉本廣雅, 河原達也.<br>
                            拡散性雑音環境下における多人数会話のマルチモーダル話者区間検出.<br>
                            日本音響学会2015春季研究発表会, 1-Q-24, 2015.
                        </li>
                        <li>
                            山口貴史, 井上昂治, 吉野幸一郎, 高梨克也, 河原達也.<br>
                            傾聴対話における相槌形態と先行発話の統語構造の関係の分析.<br>
                            人工知能学会研究会資料, SLUD-B403-4, 2015.
                        </li>
                        <li>
                            井上昂治, 若林佑幸, 吉本廣雅, 高梨克也, 河原達也.<br>
                            ポスター会話における音響・視線情報を統合した話者区間及び相槌の検出.<br>
                            情報処理学会研究報告, SLP-105-9, 2015. [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=113132&amp;item_no=1&amp;page_id=13&amp;block_id=8" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, 若林佑幸, 吉本廣雅, 河原達也.<br>
                            多人数会話における音響情報と視線情報の確率的統合による話者区間検出.<br>
                            日本音響学会2014秋季研究発表会, 2-8-4, 2014.
                        </li>
                        <li>
                            井上昂治, 若林佑幸, 吉本廣雅, 河原達也.<br>
                            多人数会話における視線情報を用いた話者区間検出.<br>
                            情報処理学会研究報告, SLP-102-1, 2014. [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=102192&amp;item_no=1&amp;page_id=13&amp;block_id=8" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            若林佑幸, 井上昂治, 河原達也, 中井駿介, 宮崎亮一, 猿渡洋.<br>
                            スマートポスターボードにおける音響情報と画像情報の統合による話者区間検出.<br>
                            日本音響学会2014春季研究発表会, 2-Q4-7, 2014.
                        </li>
                        <li>
                            中井駿介，宮崎亮一，猿渡洋，中村哲，井上昂治，若林佑幸，河原達也.<br>
                            スマートポスターボードにおける実環境を想定した複数話者分離.<br>
                            日本音響学会2014春季研究発表会, 2-Q4-8, 2014.
                        </li>
                        <li>
                            井上昂治, 松隈俊大, 黒木祥光.<br>
                            相互錐制約部分空間法.<br>
                            第15回画像の認識・理解シンポジウム (MIRU 2012), OS11-05, Aug. 2012. (オーラル)
                        </li>
                        <li>
                            黒崎正行, 伊東亮, 松尾宗明, 宮岡佑弥, 井上昂治, 江口翔馬, 尾知博, 黒木祥光,  宮崎明雄.<br>
                            暗号領域での認証を用いたJPEG 2000 画像無線伝送システム.<br>
                             電子情報通信学会 2012 年総合大会, March 2012.
                        </li>
                        <li>
                            井上昂治, 黒木祥光.<br>
                            スパース表現を用いた照明変化に頑健な顔認識に関する研究.<br>
                            平成23年度(第64回)電気関係学会九州支部連合大会, Sep. 2011. (Oral)
                        </li>
                        <li>
                            江口翔馬, 井上昂治,  黒木祥光.<br>
                            ディジタルシネマの実時間処理に向けたGPGPUによるJPEG2000の高速化.<br>
                            平成23年度(第64回)電気関係学会九州支部連合大会, Sep. 2011.
                        </li>
                        <li>
                            井上昂治, 江口翔馬, 黒木祥光, 黒崎正行, 尾知博.<br>
                            複数のGPUを用いたデジタルシネマ画像の実時間ウェーブレット変換.<br>
                            電子情報通信学会技術研究報告, SIS-111-210, pp. 45-49, Sep. 2011. [<a href="http://ci.nii.ac.jp/naid/110008899640" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            松尾正輝, 井上昂治, 黒崎正行, 黒木祥光, 斉培恒, 尾知博.<br>
                            4K デジタルシネマ無線伝送システムのための JPEG 2000 並列化.<br>
                            画像電子学会 第255回研究会, March 2011.
                        </li>
                    </ul>
                </div>
                
                 <div>
                    <h2 id="book" class="subsection-title">書籍 (Books)</h2>
                    <ul class="space-y-4 text-slate-800 publication-list">
                        <li>
                            井上昂治<br>
                            進化するヒトと機械の音声コミュニケーション Vol.2<br>
                            第2編 音声認識・合成・コミュニケーションの応用技術<br>
                            第3章 音声によるコミュニケーション技術<br>
                            共感的な傾聴対話ロボットの開発<br>
                            株式会社エヌ・ティー・エス, 2025.<br>
                            [<a href="https://nts-book.stores.jp/items/67f608e3b776cf13c7c21b7a" target="_blank" class="text-blue-600 hover:underline">出版社ページ</a>]
                        </li>
                        <li>
                            井上昂治, 河原達也.<br>
                            音声対話システム：基礎から実装まで<br>
                            オーム社, 2022.<br>
                            [<a href="https://www.ohmsha.co.jp/book/9784274229541/" target="_blank" class="text-blue-600 hover:underline">出版社ページ</a>] 
                            [<a href="https://github.com/inokoj/sds_textbook_sample" target="_blank" class="text-blue-600 hover:underline">サポートページ</a>]
                        </li>
                    </ul>
                </div>

                <div>
                    <h2 id="kaisetsu" class="subsection-title">学会誌</h2>
                     <ul class="space-y-4 text-slate-800 publication-list">
                        <li>
                            山本祐輔, 山野泰子, 井上昂治, 榊剛史, 岩澤駿.<br>
                            表紙企画 人工知能をめぐる旅: JSAI 研究会の窓 ④—先進的学習科学と工学研究会（ALST）—<br>
                            人工知能学会誌, Vol.39, No.4, pp.577-579, 2024.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/39/4/39_577/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            山野泰子, 井上昂治, 榊剛史, 山本祐輔, 岩澤駿.<br>
                            表紙企画 人工知能をめぐる旅: JSAI 研究会の窓 ③— 知識ベースシステム研究会（KBS）—<br>
                            人工知能学会誌, Vol.39, No.3, pp.435-437, 2024.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/39/3/39_435/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            榊剛史, 山野泰子, 井上昂治, 山本祐輔, 岩澤駿.<br>
                            表紙企画 人工知能をめぐる旅: JSAI 研究会の窓 ②—人工知能基本問題研究会 (FPAI)—<br>
                            人工知能学会誌, Vol.39, No.2, pp.271-273, 2024.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/39/2/39_271/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, 山野泰子, 榊剛史, 山本祐輔, 岩澤駿.<br>
                            表紙企画 人工知能をめぐる旅: JSAI 研究会の窓 ①—言語・音声理解と対話処理研究会 (SLUD)—<br>
                            人工知能学会誌, Vol.39, No.1, pp.86-89, 2024.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/39/1/39_86/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治.<br>
                            特集：編集委員 今年の抱負2024「学会誌に表紙は必要か？」<br>
                            人工知能学会誌, Vol.39, No.1, pp.11, 2024.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/39/1/39_11/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>榊剛史, 井上昂治, 山野泰子, 鳥海不二夫, 岩澤駿, 松原仁, 杉本舞, 谷口忠大.<br>
                            表紙企画 座談会：人工知能歴史絵巻の完成に際して<br>
                            人工知能学会誌, Vol.38, No.6, pp.980-989, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/38/6/38_980/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>山野泰子, 榊剛史, 井上昂治, 山本祐輔, 岩澤駿, 松原仁, 杉本舞, 谷口忠大.<br>
                            アーティクル：表紙企画 人工知能歴史絵巻：これまでのAI これからのAI ⑥ — 研究者が考える未来のAI：道は行くことでつくられる—<br>
                            人工知能学会誌, Vol.38, No.6, pp.975-979, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/38/6/38_975/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>井上昂治, 榊剛史, 山野泰子, 山本祐輔, 岩澤駿, 松原仁, 杉本舞, 谷口忠大.<br>
                            アーティクル：表紙企画 人工知能歴史絵巻：これまでのAI これからのAI ⑤ — そして生成AI ブームへAll You Need Is [MASK] —<br>
                            人工知能学会誌, Vol.38, No.5, pp.771-774, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/38/5/38_771/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>井上昂治.<br>
                            どうする対話研究<br>
                            日本バーチャルリアリティ学会誌, Vol.28, No.2, pp.33-34, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jvrsj/28/2/28_33/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>井上昂治.<br>
                            書評：セルジュ・ティスロン 著, 阿部又一郎 訳：ロボットに愛される日―AI 時代のメンタルヘルス<br>
                            人工知能学会誌, Vol.38, No.4, pp.593, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/38/4/38_593/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>榊剛史, 井上昂治, 山野泰子, 岩澤駿, 松原仁, 杉本舞, 谷口忠大.<br>
                            アーティクル：表紙企画 人工知能歴史絵巻：これまでのAI これからのAI ④ ― ブームがやって来るヤァ！ヤァ！ヤァ！―<br>
                            人工知能学会誌, Vol.38, No.4, pp.595-598, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/38/4/38_595/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>山野泰子, 井上昂治, 榊剛史, 岩澤駿, 松原仁, 杉本舞, 谷口忠大.<br>
                            アーティクル：表紙企画 人工知能歴史絵巻：これまでのAI これからのAI ③ —二度目の冬の蠢き—<br>
                            人工知能学会誌, Vol.38, No.3, pp.440-443, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/38/3/38_440/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>井上昂治, 山野泰子, 榊剛史, 岩澤駿, 松原仁, 杉本舞, 谷口忠大.<br>
                            アーティクル：表紙企画 人工知能歴史絵巻：これまでのAI これからのAI ② —冬の時代から第二次ブームへ—<br>
                            人工知能学会誌, Vol.38, No.2, pp.298-301, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/38/2/38_298/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>榊剛史,井上昂治, 山野泰子, 岩澤駿, 杉本舞, 松原仁, 谷口忠大.<br>
                            アーティクル：表紙企画 人工知能歴史絵巻：これまでのAI これからのAI —第三次AIブームを振り返って—<br>
                            人工知能学会誌, Vol.38, No.1, pp.92-95, 2023.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/38/1/38_92/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>河原達也, 井上昂治.<br>
                            アンドロイドERICAによる人間レベルの音声対話への挑戦<br>
                            日本音響学会誌, Vol.78, No.5, pp.249-256, 2022.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jasj/78/5/78_249/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                         <li>佐久間洋司, 井上昂治.<br>
                            人類の分断を克服し調和を実現するための科学技術に関する調査研究<br>
                            人工知能学会誌, Vol.36, No.6, pp.702-709, 2021.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/36/6/36_702/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>井上昂治, 岡田将吾.<br>
                            特集：「若手研究者による2050 年の未来予測～ムーンショット型研究開発 ミレニア・プログラムより～」にあたって<br>
                            人工知能学会誌, Vol.36, No.6, pp.672-673, 2021.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/36/6/36_672/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>井上昂治.<br>
                            50年経っても読まれる論文を.<br>
                            人工知能学会誌, Vol.36, No.3, pp.330, 2021.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jjsai/36/3/36_330/_article/-char/ja" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>井上昂治, 河原達也.<br>
                            アンドロイドを用いた音声対話研究<br>
                            日本音響学会誌, Vol.76, No.4, pp.236-243, 2020.<br>
                            [<a href="https://www.jstage.jst.go.jp/article/jasj/76/4/76_236/_article/-char/ja/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                    </ul>
                </div>
                
                <div>
                     <h2 id="other" class="subsection-title">Others</h2>
                     <ul class="space-y-4 text-slate-800 publication-list">
                        <li>
                            Mikey Elmers, Koji Inoue, Divesh Lala, Keiko Ochi, Tatsuya Kawahara.<br>
                            Analysis and Detection of Differences in Spoken User Behaviors between Autonomous and Wizard-of-Oz Systems.<br>
                            Workshop on Spoken Dialogue Systems for Cybernetic Avatars (SDS4CA), 2024.<br>
                            [<a href="http://www.sap.ist.i.kyoto-u.ac.jp/seminar/sds4ca/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            Zi Haur Pang, Yahui Fu, Divesh Lala, Mikey Elmers, Koji Inoue, Tatsuya Kawahara.<br>
                            Embodied Autonomous Interview System with Attentive Listening Behavior.<br>
                            Workshop on Spoken Dialogue Systems for Cybernetic Avatars (SDS4CA), 2024.<br>
                            [<a href="http://www.sap.ist.i.kyoto-u.ac.jp/seminar/sds4ca/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>Yahui Fu, 井上昂治, Chenhui Chu, 河原達也.<br>
                            Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation.<br>
                            第18回京都大学ICTイノベーション, Feb. 2024. <br>
                            [<a href="https://ict-nw.i.kyoto-u.ac.jp/ict-innovation/18th/panel/1907/" target="_blank" class="text-blue-600 hover:underline">Link</a>] <strong class="text-red-500 font-bold">優秀研究賞</strong>
                        </li>
                        <li>井上昂治.<br>
                            人を支える・人に共感する・人を超える対話システム.<br>
                            日本音響学会 学生・若手フォーラム ビギナーズセミナー in KANSAI ～すべての道は音声対話に通ず～(第33回関西支部談話会), March 2023.<br>
                            [<a href="http://asj-fresh.acoustics.jp/event/2023-02-2965" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>Tatsuya Kawahara, Koji Inoue, Divesh Lala.<br>
                            Intelligent Conversational Android ERICA Applied to Attentive Listening and Job Interview<br>
                            arXiv:2105.00403, 2021.<br>
                            [<a href="https://arxiv.org/abs/2105.00403" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>井上昂治.<br>
                            コロナ禍における多人数音声対話システムの被験者実験.<br>
                            電子情報通信学会 VNV研究会 第15回年次大会, March 2021.
                        </li>
                        <li>
                            井上昂治.<br>
                            アンドロイドと音声対話システムの融合 -深層ヒューマンロボットインタラクションの実現に向けて-.<br>
                            日本ロボット学会ヒューロビント研究専門委員会 若手ロボティクス研究会, Jan. 2021.
                        </li>
                        <li>有本庸浩, 佐藤志貴, 井上昂治, 山本賢太, 赤間怜奈.<br>
                            国際会議報告（SIGDIAL, ACL, ICMI, Interspeech, EMNLP）.<br>
                            人工知能学会 言語・音声理解と対話処理研究会 (SLUD) 第90回研究会 (第11回対話システムシンポジウム), Dec. 2020.
                        </li>
                        <li>
                            井上昂治.<br>
                            アンドロイドERICAによる傾聴対話システム.<br>
                            第４回 JST ERATO石黒共生HRIプロジェクトシンポジウム, Aug. 2020.<br>
                            [<a href="https://sites.google.com/irl.sys.es.osaka-u.ac.jp/shri-symposium-2020/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>山本賢太, 井上昂治, 河原達也.<br>
                            多様な応答を生成する傾聴対話システム -ロボットがあなたの話を聴きます-.<br>
                            第14回京都大学ICTイノベーション, Feb. 2020. <br>
                            [<a href="http://ict-nw.i.kyoto-u.ac.jp/ict-innovation/14th/panel/panel.php?id=19" target="_blank" class="text-blue-600 hover:underline">Link</a>] <strong class="text-red-500 font-bold">優秀研究賞</strong>
                        </li>
                        <li>井上昂治.<br>
                            博士の学位取得後の大学に就職するキャリア.<br>
                            IEEE 関西支部 Young Professionals 博士課程のキャリアについて語る会, Sep. 2019.
                        </li>
                        <li>石井亮, 井上昂治, 千葉祐弥, 角森唯子, 成松宏美, 福田悠人, 増村亮.<br>
                            国際会議報告（SIGDIAL, ACL, IJCAI-ECAI, COLING, Interspeech, ICMI, EMNLP, IVA, SEMDIAL）.<br>
                            人工知能学会 言語・音声理解と対話処理研究会 (SLUD) 第84回研究会 (第9回対話システムシンポジウム), Nov. 2018.<br>
                            [<a href="https://github.com/jsai-slud/sig-slud/raw/master/material/84th/DSS9_conference_reports.pdf" target="_blank" class="text-blue-600 hover:underline">PDF</a>]
                        </li>
                        <li>
                            井上昂治.<br>
                            対話におけるエンゲージメント推定.<br>
                            第２回 JST ERATO石黒共生HRIプロジェクトシンポジウム, Aug. 2018.<br>
                            [<a href="https://sites.google.com/view/erato-shri-symposium2018/" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, 河原達也.<br>
                            会話エンゲージメントの自動認識 -相手が会話に興味があるかがわかります-.<br>
                            第12回京都大学ICTイノベーション, March 2018.<br>
                            [<a href="http://ict-nw.i.kyoto-u.ac.jp/ict-innovation/12th/panel/panel.php?id=10" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治.<br>
                             聞き手のふるまいに着目した対話エンゲージメントの分析と予測.<br>
                             電子情報通信学会第52回VNV研究会 May. 2016.
                        </li>
                        <li>
                             井上昂治.<br>
                             対話ロボットは空気を読めるか？.<br>
                            日本音響学会北陸支部第7回音響セミナー ビギナー成果発表会, Dec. 2015.<br>
                            [<a href="http://www.ais.jaist.ac.jp/asj-hokuriku/onkyo-seminar7th.html" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            山口貴史, 井上昂治, 吉野幸一郎, 高梨克也, Nigel G. Ward, 河原達也.<br>
                            多様な形態の相槌をうつ傾聴対話システム.<br>
                            第18回日本音響学会関西支部若手研究者交流研究発表会, 11, Dec. 2015. <strong class="text-red-500 font-bold">優秀奨励賞</strong>
                        </li>
                        <li>
                            井上昂治.<br>
                            ポスター会話における音響・視線情報を統合した話者区間及び相槌の検出.<br>
                            第41回関西音声合同ゼミ, A-10, July 2015. 
                        </li>
                        <li>
                            Koji Inoue.<br>
                            Multi-modal conversational analysis in poster sessions.<br>
                             Multi-modal Interaction Workshop, April 2015.<br>
                              [<a href="http://www.ii.ist.i.kyoto-u.ac.jp/?p=5616" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, 吉本廣雅, 河原達也.<br>
                            スマートポスターボード -聴衆の反応のセンシング-.<br>
                            第9回京都大学ICTイノベーション, March 2015.<br>
                            [<a href="http://ict-nw.i.kyoto-u.ac.jp/ict-innovation/2015/panel/panel.php?id=1" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, 若林佑幸, 吉本廣雅, 河原達也.<br>
                            多人数会話における視線情報を用いた話者区間検出.<br>
                            第17回日本音響学会関西支部若手研究者交流研究発表会, 10, Dec. 2014.<br>
                             [<a href="http://asj-kansai.acoustics.jp/event/wakate_2014_report.html" target="_blank" class="text-blue-600 hover:underline">Link</a>]　<strong class="text-red-500 font-bold">優秀奨励賞</strong>
                        </li>
                        <li>
                            若林佑幸, 中山雅人, 西浦敬信, 井上昂治, 吉本廣雅, 河原達也.<br>
                            マルチモーダル話者ダイアライゼーション ～Who speaks when?～.<br>
                            第17回日本音響学会関西支部若手研究者交流研究発表会, 12, Dec. 2014.
                        </li>
                        <li>
                            井上昂治.<br>
                            多人数会話における視線情報を用いた話者区間検出.<br>
                            第39回関西音声合同ゼミ, B.3, July 2014.<br>
                             [<a href="http://www.aspl.is.ritsumei.ac.jp/kansai/2014/index.html#presen_program" target="_blank" class="text-blue-600 hover:underline">Link</a>]
                        </li>
                        <li>
                            井上昂治, 河原達也.<br>
                            スマートポスターボード -聴衆の反応のセンシング-.<br>
                            第8回京都大学ICTイノベーション, Dec. 2013.
                        </li>
                    </ul>
                </div>

            </section>
        </main>

        <footer class="text-center mt-16 py-8 border-t border-slate-200">
            <p class="text-slate-500">&copy; 2025 Koji Inoue. All rights reserved. </p>
        </footer>

    </div>

</body>
</html>
